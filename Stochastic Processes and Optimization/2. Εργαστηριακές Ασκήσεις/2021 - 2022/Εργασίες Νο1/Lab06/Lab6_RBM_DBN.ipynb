{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5q7C447X8Up"
   },
   "source": [
    "<h1><b>Restricted Boltzmann Machine και Deep Belief Network</b></h1>\n",
    "<p align=\"justify\">Στην συγκεκριμένη άσκηση θα μελετήσετε τον τρόπο λειτουργίας μιας <i>RBM (<a href=\"https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine\">Restricted Boltzmann Machine</a>)</i> καθώς και των <i>DBN (<a href=\"https://en.wikipedia.org/wiki/Deep_belief_network\">Deep Belief Network</a>)</i>, χρησιμοποιώντας το έτοιμο πρόγραμμα που σας δίνεται.Το συγκεκριμένο πρόγραμμα αξιοποιεί το <a href=\"https://en.wikipedia.org/wiki/MNIST_database\">dataset του <i>MNIST</i></a>, όπου είναι μια μεγάλη βάση δεδομένων με χειρόγραφα ψηφία που χρησιμοποιείται συνήθως για την εκπαίδευση διαφόρων συστημάτων επεξεργασίας εικόνας. Για την άσκηση, θα πρέπει να χρησιμοποιήσετε το αρχείο <i>mnist_original.mat</i>, το οποίο είναι διαθέσιμο από <a href=\"https://www.kaggle.com/datasets/avnishnish/mnist-original?resource=download\">εδώ</a>.</p>\n",
    "<p align=\"justify\">Μία αρκετά σημαντική εφαρμογή της <i>RBM</i> είναι η εξαγωγή χαρακτηριστικών (feature representation) από ένα dataset με σκοπό την αναπαράσταση της εισόδου (ορατοί νευρώνες) με ένα διάνυσμα μικρότερης διάστασης (κρυφοί νευρώνες). Στη συγκεκριμένη άσκηση θα συγκρίνετε την ακρίβεια ενός ταξινομητή ψηφίων με τη χρήση του αλγορίθμου <i>Logistic Regression</i>, όταν εκείνος δέχεται ως είσοδο το dataset (i) χωρίς να έχει υποστεί επεξεργασία από το <i>RBM</i>, (ii) αφου υποστεί επεξεργασία από το <i>RBM</i>, (iii) με τη χρήση <i>DBN</i>, δηλαδή δύο stacked <i>RBM</i>.</p>\n",
    "<p align=\"justify\"> Με βάση τον κώδικα που σας έχει δοθεί, καλείστε να απαντήσετε στα παρακάτω ερωτήματα:</p>\n",
    "<ul>\n",
    "<li>Να περιγράψετε σύντομα τον τρόπο λειτουργίας μιας <i>RBM</i>. Τι διαφορές έχει σε σχέση με μία <i> Μηχανή Boltzmann</i>;</li>\n",
    "<li>Ποια είναι η λογική των <i>DBN</i> και σε τι προβλήματα τα αξιοποιούμε;</li>\n",
    "<li>Να αναφέρετε τις βασικότερες εφαρμογές των <i>RBM</i> και <i>DBN</i>.</li>\n",
    "<li>Εκτός από <i>RBM</i>, τι άλλα μοντέλα μπορούν να χρησιμοποιηθούν για να δημιουργήσουν <i>DBN</i>.</li>\n",
    "<li>Συγκρίνετε τα αποτελέσματα της ταξινόμησης με τον αλγόριθμo <i>Logistic Regression</i> χωρίς τη χρήση <i>RBM</i> σε σχέση με τα αποτελέσματα της ταξινόμησης που έχει χρησιμοποιηθεί η <i>RBM</i> καθώς και με αυτή όπου χρησιμοποιούνται <i>RBM</i> και <i>DBN</i> για την εξαγωγή των χαρακτηριστικών. Τι παρατηρείτε ως προς την ακρίβεια των αποτελεσμάτων;</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Z6OQ6-N8ajZJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\rigas\\anaconda3\\envs\\DSMLEnv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   10.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.96      0.96       995\n",
      "         1.0       0.96      0.98      0.97      1121\n",
      "         2.0       0.90      0.90      0.90      1015\n",
      "         3.0       0.90      0.88      0.89      1033\n",
      "         4.0       0.93      0.92      0.92       976\n",
      "         5.0       0.90      0.88      0.89       884\n",
      "         6.0       0.94      0.94      0.94       999\n",
      "         7.0       0.92      0.93      0.92      1034\n",
      "         8.0       0.88      0.86      0.87       923\n",
      "         9.0       0.89      0.90      0.90      1020\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.92      0.92      0.92     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n",
      "\n",
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -142.32, time = 5.72s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -118.06, time = 8.31s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -106.05, time = 8.50s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -96.80, time = 8.41s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -93.82, time = 8.55s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -87.63, time = 8.46s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -84.40, time = 8.39s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -82.97, time = 8.46s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -81.03, time = 8.44s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -79.13, time = 8.42s\n",
      "[BernoulliRBM] Iteration 11, pseudo-likelihood = -76.25, time = 8.47s\n",
      "[BernoulliRBM] Iteration 12, pseudo-likelihood = -76.30, time = 8.42s\n",
      "[BernoulliRBM] Iteration 13, pseudo-likelihood = -74.80, time = 8.50s\n",
      "[BernoulliRBM] Iteration 14, pseudo-likelihood = -73.77, time = 8.47s\n",
      "[BernoulliRBM] Iteration 15, pseudo-likelihood = -73.44, time = 8.42s\n",
      "[BernoulliRBM] Iteration 16, pseudo-likelihood = -72.25, time = 8.49s\n",
      "[BernoulliRBM] Iteration 17, pseudo-likelihood = -71.12, time = 8.47s\n",
      "[BernoulliRBM] Iteration 18, pseudo-likelihood = -70.19, time = 8.43s\n",
      "[BernoulliRBM] Iteration 19, pseudo-likelihood = -69.84, time = 8.42s\n",
      "[BernoulliRBM] Iteration 20, pseudo-likelihood = -67.89, time = 8.47s\n",
      "[BernoulliRBM] Iteration 21, pseudo-likelihood = -69.06, time = 8.41s\n",
      "[BernoulliRBM] Iteration 22, pseudo-likelihood = -68.52, time = 8.38s\n",
      "[BernoulliRBM] Iteration 23, pseudo-likelihood = -67.39, time = 8.56s\n",
      "[BernoulliRBM] Iteration 24, pseudo-likelihood = -67.18, time = 8.77s\n",
      "[BernoulliRBM] Iteration 25, pseudo-likelihood = -67.74, time = 8.37s\n",
      "[BernoulliRBM] Iteration 26, pseudo-likelihood = -67.13, time = 8.47s\n",
      "[BernoulliRBM] Iteration 27, pseudo-likelihood = -66.49, time = 8.44s\n",
      "[BernoulliRBM] Iteration 28, pseudo-likelihood = -66.30, time = 8.62s\n",
      "[BernoulliRBM] Iteration 29, pseudo-likelihood = -66.94, time = 8.35s\n",
      "[BernoulliRBM] Iteration 30, pseudo-likelihood = -65.65, time = 8.38s\n",
      "[BernoulliRBM] Iteration 31, pseudo-likelihood = -65.56, time = 8.50s\n",
      "[BernoulliRBM] Iteration 32, pseudo-likelihood = -63.49, time = 8.52s\n",
      "[BernoulliRBM] Iteration 33, pseudo-likelihood = -65.00, time = 8.48s\n",
      "[BernoulliRBM] Iteration 34, pseudo-likelihood = -64.53, time = 8.55s\n",
      "[BernoulliRBM] Iteration 35, pseudo-likelihood = -65.64, time = 8.38s\n",
      "[BernoulliRBM] Iteration 36, pseudo-likelihood = -64.58, time = 8.45s\n",
      "[BernoulliRBM] Iteration 37, pseudo-likelihood = -64.67, time = 8.49s\n",
      "[BernoulliRBM] Iteration 38, pseudo-likelihood = -62.46, time = 8.38s\n",
      "[BernoulliRBM] Iteration 39, pseudo-likelihood = -63.31, time = 8.39s\n",
      "[BernoulliRBM] Iteration 40, pseudo-likelihood = -62.46, time = 8.43s\n",
      "[BernoulliRBM] Iteration 41, pseudo-likelihood = -62.04, time = 8.36s\n",
      "[BernoulliRBM] Iteration 42, pseudo-likelihood = -63.13, time = 8.43s\n",
      "[BernoulliRBM] Iteration 43, pseudo-likelihood = -62.29, time = 8.41s\n",
      "[BernoulliRBM] Iteration 44, pseudo-likelihood = -62.26, time = 8.28s\n",
      "[BernoulliRBM] Iteration 45, pseudo-likelihood = -63.13, time = 8.38s\n",
      "[BernoulliRBM] Iteration 46, pseudo-likelihood = -61.43, time = 8.45s\n",
      "[BernoulliRBM] Iteration 47, pseudo-likelihood = -61.77, time = 8.47s\n",
      "[BernoulliRBM] Iteration 48, pseudo-likelihood = -61.12, time = 8.50s\n",
      "[BernoulliRBM] Iteration 49, pseudo-likelihood = -61.83, time = 8.45s\n",
      "[BernoulliRBM] Iteration 50, pseudo-likelihood = -63.04, time = 8.34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\rigas\\anaconda3\\envs\\DSMLEnv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   10.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.98       995\n",
      "         1.0       0.98      0.99      0.99      1121\n",
      "         2.0       0.97      0.97      0.97      1015\n",
      "         3.0       0.97      0.97      0.97      1033\n",
      "         4.0       0.98      0.97      0.97       976\n",
      "         5.0       0.97      0.97      0.97       884\n",
      "         6.0       0.98      0.98      0.98       999\n",
      "         7.0       0.97      0.98      0.97      1034\n",
      "         8.0       0.96      0.95      0.96       923\n",
      "         9.0       0.96      0.96      0.96      1020\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "\n",
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -140.06, time = 5.81s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -117.58, time = 8.32s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -104.38, time = 8.34s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -97.84, time = 8.41s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -91.86, time = 8.51s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -88.94, time = 8.43s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -86.32, time = 8.49s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -81.64, time = 8.62s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -79.79, time = 8.35s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -79.43, time = 8.32s\n",
      "[BernoulliRBM] Iteration 11, pseudo-likelihood = -77.66, time = 8.42s\n",
      "[BernoulliRBM] Iteration 12, pseudo-likelihood = -76.34, time = 8.38s\n",
      "[BernoulliRBM] Iteration 13, pseudo-likelihood = -75.06, time = 8.39s\n",
      "[BernoulliRBM] Iteration 14, pseudo-likelihood = -75.21, time = 8.46s\n",
      "[BernoulliRBM] Iteration 15, pseudo-likelihood = -71.79, time = 8.53s\n",
      "[BernoulliRBM] Iteration 16, pseudo-likelihood = -74.21, time = 8.54s\n",
      "[BernoulliRBM] Iteration 17, pseudo-likelihood = -72.02, time = 8.47s\n",
      "[BernoulliRBM] Iteration 18, pseudo-likelihood = -71.85, time = 8.38s\n",
      "[BernoulliRBM] Iteration 19, pseudo-likelihood = -71.20, time = 8.50s\n",
      "[BernoulliRBM] Iteration 20, pseudo-likelihood = -68.50, time = 8.47s\n",
      "[BernoulliRBM] Iteration 21, pseudo-likelihood = -67.91, time = 8.51s\n",
      "[BernoulliRBM] Iteration 22, pseudo-likelihood = -68.17, time = 8.42s\n",
      "[BernoulliRBM] Iteration 23, pseudo-likelihood = -68.01, time = 8.39s\n",
      "[BernoulliRBM] Iteration 24, pseudo-likelihood = -67.89, time = 8.43s\n",
      "[BernoulliRBM] Iteration 25, pseudo-likelihood = -66.16, time = 8.34s\n",
      "[BernoulliRBM] Iteration 26, pseudo-likelihood = -66.87, time = 8.41s\n",
      "[BernoulliRBM] Iteration 27, pseudo-likelihood = -66.10, time = 8.39s\n",
      "[BernoulliRBM] Iteration 28, pseudo-likelihood = -65.80, time = 8.56s\n",
      "[BernoulliRBM] Iteration 29, pseudo-likelihood = -64.92, time = 8.60s\n",
      "[BernoulliRBM] Iteration 30, pseudo-likelihood = -65.15, time = 8.53s\n",
      "[BernoulliRBM] Iteration 31, pseudo-likelihood = -66.10, time = 8.47s\n",
      "[BernoulliRBM] Iteration 32, pseudo-likelihood = -66.08, time = 8.40s\n",
      "[BernoulliRBM] Iteration 33, pseudo-likelihood = -64.80, time = 8.41s\n",
      "[BernoulliRBM] Iteration 34, pseudo-likelihood = -63.18, time = 8.42s\n",
      "[BernoulliRBM] Iteration 35, pseudo-likelihood = -64.66, time = 8.43s\n",
      "[BernoulliRBM] Iteration 36, pseudo-likelihood = -63.55, time = 8.41s\n",
      "[BernoulliRBM] Iteration 37, pseudo-likelihood = -63.14, time = 8.43s\n",
      "[BernoulliRBM] Iteration 38, pseudo-likelihood = -64.24, time = 8.47s\n",
      "[BernoulliRBM] Iteration 39, pseudo-likelihood = -64.33, time = 8.43s\n",
      "[BernoulliRBM] Iteration 40, pseudo-likelihood = -63.81, time = 8.47s\n",
      "[BernoulliRBM] Iteration 41, pseudo-likelihood = -61.55, time = 8.42s\n",
      "[BernoulliRBM] Iteration 42, pseudo-likelihood = -62.65, time = 8.46s\n",
      "[BernoulliRBM] Iteration 43, pseudo-likelihood = -62.06, time = 8.45s\n",
      "[BernoulliRBM] Iteration 44, pseudo-likelihood = -61.54, time = 8.62s\n",
      "[BernoulliRBM] Iteration 45, pseudo-likelihood = -61.55, time = 8.57s\n",
      "[BernoulliRBM] Iteration 46, pseudo-likelihood = -62.19, time = 8.37s\n",
      "[BernoulliRBM] Iteration 47, pseudo-likelihood = -62.00, time = 8.51s\n",
      "[BernoulliRBM] Iteration 48, pseudo-likelihood = -62.09, time = 8.47s\n",
      "[BernoulliRBM] Iteration 49, pseudo-likelihood = -62.14, time = 8.44s\n",
      "[BernoulliRBM] Iteration 50, pseudo-likelihood = -61.63, time = 8.45s\n",
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -282.97, time = 5.99s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -251.64, time = 8.28s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -236.17, time = 8.50s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -225.68, time = 8.38s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -222.00, time = 8.54s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -219.17, time = 8.42s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -213.50, time = 8.38s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -213.05, time = 8.47s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -209.69, time = 8.36s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -208.00, time = 8.39s\n",
      "[BernoulliRBM] Iteration 11, pseudo-likelihood = -204.94, time = 8.40s\n",
      "[BernoulliRBM] Iteration 12, pseudo-likelihood = -204.37, time = 8.42s\n",
      "[BernoulliRBM] Iteration 13, pseudo-likelihood = -203.18, time = 8.40s\n",
      "[BernoulliRBM] Iteration 14, pseudo-likelihood = -200.08, time = 8.33s\n",
      "[BernoulliRBM] Iteration 15, pseudo-likelihood = -199.12, time = 8.43s\n",
      "[BernoulliRBM] Iteration 16, pseudo-likelihood = -200.77, time = 8.32s\n",
      "[BernoulliRBM] Iteration 17, pseudo-likelihood = -200.61, time = 8.38s\n",
      "[BernoulliRBM] Iteration 18, pseudo-likelihood = -196.91, time = 8.30s\n",
      "[BernoulliRBM] Iteration 19, pseudo-likelihood = -197.30, time = 8.31s\n",
      "[BernoulliRBM] Iteration 20, pseudo-likelihood = -196.57, time = 8.33s\n",
      "[BernoulliRBM] Iteration 21, pseudo-likelihood = -194.62, time = 8.32s\n",
      "[BernoulliRBM] Iteration 22, pseudo-likelihood = -196.08, time = 8.35s\n",
      "[BernoulliRBM] Iteration 23, pseudo-likelihood = -194.45, time = 8.22s\n",
      "[BernoulliRBM] Iteration 24, pseudo-likelihood = -195.70, time = 8.34s\n",
      "[BernoulliRBM] Iteration 25, pseudo-likelihood = -196.21, time = 8.29s\n",
      "[BernoulliRBM] Iteration 26, pseudo-likelihood = -195.86, time = 8.29s\n",
      "[BernoulliRBM] Iteration 27, pseudo-likelihood = -195.33, time = 8.53s\n",
      "[BernoulliRBM] Iteration 28, pseudo-likelihood = -193.21, time = 8.57s\n",
      "[BernoulliRBM] Iteration 29, pseudo-likelihood = -192.08, time = 8.28s\n",
      "[BernoulliRBM] Iteration 30, pseudo-likelihood = -192.64, time = 8.70s\n",
      "[BernoulliRBM] Iteration 31, pseudo-likelihood = -193.65, time = 8.50s\n",
      "[BernoulliRBM] Iteration 32, pseudo-likelihood = -194.12, time = 8.25s\n",
      "[BernoulliRBM] Iteration 33, pseudo-likelihood = -192.13, time = 8.53s\n",
      "[BernoulliRBM] Iteration 34, pseudo-likelihood = -193.03, time = 8.71s\n",
      "[BernoulliRBM] Iteration 35, pseudo-likelihood = -189.82, time = 8.33s\n",
      "[BernoulliRBM] Iteration 36, pseudo-likelihood = -190.77, time = 8.30s\n",
      "[BernoulliRBM] Iteration 37, pseudo-likelihood = -191.97, time = 8.30s\n",
      "[BernoulliRBM] Iteration 38, pseudo-likelihood = -191.60, time = 8.29s\n",
      "[BernoulliRBM] Iteration 39, pseudo-likelihood = -191.36, time = 8.26s\n",
      "[BernoulliRBM] Iteration 40, pseudo-likelihood = -191.75, time = 8.21s\n",
      "[BernoulliRBM] Iteration 41, pseudo-likelihood = -191.66, time = 8.30s\n",
      "[BernoulliRBM] Iteration 42, pseudo-likelihood = -192.10, time = 8.28s\n",
      "[BernoulliRBM] Iteration 43, pseudo-likelihood = -191.20, time = 8.35s\n",
      "[BernoulliRBM] Iteration 44, pseudo-likelihood = -191.36, time = 8.37s\n",
      "[BernoulliRBM] Iteration 45, pseudo-likelihood = -191.86, time = 8.30s\n",
      "[BernoulliRBM] Iteration 46, pseudo-likelihood = -190.78, time = 8.47s\n",
      "[BernoulliRBM] Iteration 47, pseudo-likelihood = -191.36, time = 8.34s\n",
      "[BernoulliRBM] Iteration 48, pseudo-likelihood = -189.02, time = 8.35s\n",
      "[BernoulliRBM] Iteration 49, pseudo-likelihood = -191.18, time = 8.44s\n",
      "[BernoulliRBM] Iteration 50, pseudo-likelihood = -191.21, time = 8.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\rigas\\anaconda3\\envs\\DSMLEnv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   10.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99       995\n",
      "         1.0       0.99      0.99      0.99      1121\n",
      "         2.0       0.98      0.98      0.98      1015\n",
      "         3.0       0.98      0.97      0.98      1033\n",
      "         4.0       0.98      0.97      0.97       976\n",
      "         5.0       0.98      0.97      0.97       884\n",
      "         6.0       0.99      0.99      0.99       999\n",
      "         7.0       0.97      0.98      0.98      1034\n",
      "         8.0       0.96      0.97      0.97       923\n",
      "         9.0       0.96      0.97      0.96      1020\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# source: https://devdreamz.com/question/905929-stacking-rbms-to-create-deep-belief-network-in-sklearn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def norm(arr):\n",
    "    arr = arr.astype(float)\n",
    "    arr -= arr.min()\n",
    "    arr /= arr.max()\n",
    "    return arr\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # load MNIST data set\n",
    "    mnist = loadmat(\"mnist-original.mat\")\n",
    "    X, Y = mnist[\"data\"].T, mnist[\"label\"][0]\n",
    "\n",
    "    # normalize inputs to 0-1 range\n",
    "    X = norm(X)\n",
    "\n",
    "    # split into train, validation, and test data sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,       Y,       test_size=10000, random_state=0)\n",
    "    X_train, X_val,  Y_train, Y_val  = train_test_split(X_train, Y_train, test_size=10000, random_state=0)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # set hyperparameters\n",
    "\n",
    "    learning_rate = 0.02 \n",
    "    total_units   =  800 \n",
    "    total_epochs  =   50\n",
    "    batch_size    =  128\n",
    "\n",
    "    C = 100. # optimum for benchmark model according to sklearn docs: https://scikit-learn.org/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html#sphx-glr-auto-examples-neural-networks-plot-rbm-logistic-classification-py)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # construct models\n",
    "\n",
    "    # RBM\n",
    "    rbm = BernoulliRBM(n_components=total_units, learning_rate=learning_rate, batch_size=batch_size, n_iter=total_epochs, verbose=1)\n",
    "\n",
    "    # \"output layer\"\n",
    "    logistic = LogisticRegression(C=C, solver='lbfgs', multi_class='multinomial', max_iter=200, verbose=1)\n",
    "\n",
    "    models = []\n",
    "    models.append(Pipeline(steps=[('logistic', clone(logistic))]))                                              # base model / benchmark\n",
    "    models.append(Pipeline(steps=[('rbm1', clone(rbm)), ('logistic', clone(logistic))]))                        # single RBM\n",
    "    models.append(Pipeline(steps=[('rbm1', clone(rbm)), ('rbm2', clone(rbm)), ('logistic', clone(logistic))]))  # RBM stack / DBN\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # train and evaluate models\n",
    "\n",
    "    for model in models:\n",
    "        # train\n",
    "        model.fit(X_train, Y_train)\n",
    "\n",
    "        # evaluate using validation set\n",
    "        print(\"Model performance:\\n%s\\n\" % (\n",
    "            classification_report(Y_val, model.predict(X_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Απαντήσεις\n",
    "\n",
    "- <font color='#486393'>Πριν γίνει αναφορά στις Restricted Boltzmann Machines (RBMs), θα πρέπει πρώτα να συζητηθούν συνοπτικά οι καθιερωμένες Μηχανές Boltzmann (Boltzmann Machines - BMs), αφού αυτές αποτελούν τη γενίκευση των RBMs. Μια Μηχανή Boltzmann αποτελεί ένα είδος ρηχού νευρωνικού δικτύου το οποίο αποτελείται από ένα σύνολο φανερών νευρώνων (φανερό επίπεδο) και ένα σύνολο κρυφών νευρώνων (κρυφό επίπεδο) και μπορεί να απεικονιστεί ως ένας διμερής γράφος. Επιπλέον, χαρακτηρίζεται από συμμετρικές συνάψεις, δηλαδή μεταξύ δύο νευρώνων $i$, $j$ υπάρχουν και η σύναψη $i \\to j$, αλλά και η σύναψη $j \\to i$, με ίσα συναπτικά βάρη, $w_{ij} = w_{ji}$. Η Μηχανή Boltzmann χρησιμοποιείται για την εκμάθηση χαρακτηριστικών μιας άγνωστης κατανομής πιθανότητας, αξιοποιώντας διαθέσιμα δείγματα από την κατανομή αυτή. Γενικά, η διαδικασία αυτή είναι περίπλοκη και χρονοβόρα, μπορεί όμως να απλοποιηθεί και να επιταχυνθεί εάν κανείς επιβάλλει ορισμένους περιορισμούς στην τοπολογία του δικτύου, εξ ου και η έννοια της \"Restricted\" Boltzmann Machine.</font>\n",
    "\n",
    "<font color='#486393'>Μια RBM αποτελεί υποπερίπτωση Μηχανής Boltzmann, στην οποία δεν υπάρχουν συνάψεις μεταξύ νευρώνων που ανήκουν στο ίδιο επίπεδο. Με άλλα λόγια, οι συνάψεις με εν γένει μη μηδενικά συναπτικά βάρη είναι μόνο αυτές μεταξύ ενός κρυφού και ενός φανερού νευρώνα. Στην παρακάτω εικόνα φαίνονται δύο μηχανές Boltzmann που αποτελούνται από ένα φανερό επίπεδο 3 νευρώνων και ένα κρυφό επίπεδο 2 νευρώνων. Η βασική τους διαφορά, η οποία καθιστά τη μηχανή στα δεξιά ως RBM, έγκειται στον περιορισμό αυτό σχετικά με τις συνάψεις.</font><br>\n",
    "\n",
    "<center><img src=\"RBMImage.png\" width=\"600\" /></center><br>\n",
    "\n",
    "<font color='#486393'>Σε ό,τι έχει να κάνει με την αρχή λειτουργίας τους, για κάθε στοιχείο $\\mathbf{x}_v$ του δείγματος, η RBM προσπαθεί σε βάθος $E$ εποχών μιας επαναληπτικής διαδικασίας να μάθει να ανακατασκευάζει το στοιχείο αυτό. Αυτός είναι και ο λόγος για τον οποίο μιλάμε για παραγωγική (generative) διαδικασία. Με τον τρόπο αυτό, η RBM εκπαιδεύεται ώστε το ορατό της επίπεδο να πραγματοποιεί δειγματοληψία από μια κατανομή Gibbs, η οποία όμως προσεγγίζει όσο το δυνατό περισσότερο την άγνωστη κατανομή από την οποία προκύπτουν τα δειγματικά σημεία του συνόλου εκπαίδευσης.</font>\n",
    "\n",
    "<font color='#486393'>Στην πράξη, η διαδικασία εκπαίδευσης της RBM ξεκινά με την αρχικοποίηση όλων των μη μηδενικών συναπτικών βαρών σε τυχαίες τιμές. Κάθε νευρώνας μπορεί να βρίσκεται σε μία από δύο επιτρεπτές καταστάσεις, έστω 0 και 1 (εξ ου και ο χαρακτηρισμός «Bernoulli» RBM στον παραπάνω κώδικα) και ενεργοποιείται μέσω της σιγμοειδούς συνάρτησης. Στην αρχή κάθε εποχής, $e$, με $e \\in \\left\\{0,\\dots,E\\right\\}$, ο αλγόριθμος λαμβάνει ως είσοδο των ορατών νευρώνων κάθε στοιχείο $\\mathbf{x}_v$ του δείγματος και εν παραλλήλω το προωθεί στους κρυφούς νευρώνες, όπου υπολογίζεται η ενεργοποίησή τους (forward pass). Από εκεί, πραγματοποιείται δειγματοληψία κατά Gibbs και οι τιμές που παράγονται προωθούνται εν παραλλήλω στους ορατούς νευρώνες (μέσω των ίδιων συναπτικών βαρών με πριν λόγω συμμετρίας), όπου υπολογίζεται η ενεργοποίησή τους (backward pass), $\\tilde{\\mathbf{x}}_v$. Με το πέρας της εποχής, τα συναπτικά βάρη ανανεώνονται έτσι, ώστε να μεγιστοποιείται ο λογάριθμος της πιθανοφάνειας των ανεξάρτητων μεταξύ τους δειγματικών σημείων (το οποίο ισοδυναμεί με την ελαχιστοποίηση των διαφοροποιήσεων ανάμεσα σε $\\mathbf{x}_v$ και $\\tilde{\\mathbf{x}}_v$).</font>\n",
    "\n",
    "<font color='#486393'>Ένα σημείο στο οποίο αξίζει κανείς να σταθεί είναι το γεγονός πως η δειγματοληψία από την κατανομή Gibbs μπορεί να πραγματοποιηθεί γρήγορα (μέσω της προαναφερθείσας παραλληλοποίησης) ακριβώς επειδή οι κρυφοί νευρώνες είναι μεταξύ τους ανεξάρτητοι δεδομένων των ορατών και αντιστρόφως. Αυτή είναι η αφορμή για τη διαφοροποίηση των RBM από τις καθιερωμένες Μηχανές Boltzmann. Παρ' όλα αυτά, η δειγματοληψία κατά Gibbs κατά το backward pass δε μπορεί να γίνει αμερόληπτα εάν δεν έχει επέλθει ισορροπία, εάν δηλαδή το δίκτυο δεν έχει μεταβεί στην κατάσταση που περιγράφεται από την αναλλοίωτη κατανομή Markov του. Για το σκοπό αυτό επιστρετεύονται διάφοροι αλγόριθμοι, όπως είναι ο Contrastive Divergence (CD) που συζητήθηκε στο μάθημα. Μέσω αυτών, παρότι δεν επιτυγχάνεται η ελαχιστοποίηση της πιθανοφάνειας, επιτυγχάνεται η ελαχιστοποίηση της απόκλισης Kullback-Leibler μεταξύ κατανομής Gibbs και άγνωστης κατανομής, δίνοντας τελικά ικανοποιητικά αποτελέσματα.</font>\n",
    "\n",
    "<!--<font color='#486393'>αποτελείται από ένα σύνολο βημάτων, $t$, με $t \\in \\left\\{0,\\dots,T\\right\\}$. Κάθε βήμα αποτελείται από διαδοχικές παραγωγές τιμών: πρώτα από όλους τους ορατούς νευρώνες, οι οποίες προωθούνται στους κρυφούς νευρώνες για να παράξουν στοιχεία $\\mathbf{x}_h^{(t)}\\left(e\\right)$, και κατόπιν από τους κρυφούς νευρώνες οι οποίες προωθούνται στους ορατούς νευρώνες για να παράξουν στοιχεία $\\mathbf{x}_v^{(t+1)}\\left(e\\right)$. Προφανώς, η αρχικοποίηση γίνεται ως $\\mathbf{x}_v^{(0)}\\left(0\\right) = \\mathbf{x}_v$. Κάθε νευρώνας μπορεί να βρίσκεται σε μία από δύο επιτρεπτές καταστάσεις, έστω 0 και 1, εξ ου και ο χαρακτηρισμός «Bernoulli» RBM στον παραπάνω κώδικα, και ενεργοποιείται μέσω της σιγμοειδούς συνάρτησης. Μετά από $T$ βήματα, οι διαφοροποιήσεις ως προς τα συναπτικά βάρη της κατάστασης κατά την αρχή της εποχής, $\\mathbf{x}_v^{(0)}\\left(e\\right)$, και κατά το τέλος της εποχής, $\\mathbf{x}_v^{(T)}\\left(e\\right)$, ελαχιστοποιούνται, προκειμένου να μεγιστοποιηθεί ο λογάριθμος της πιθανοφάνειας όλων των ανεξάρτητων δειγματικών σημείων. Έτσι, με το πέρας $E$ εποχών, επιτυγχάνεται η δημιουργία καταστάσεων $\\mathbf{x}_v^{(T)}\\left(E\\right)$ οι οποίες αντιστοιχούν σε μια κατανομή Gibbs, η οποία με τη σειρά της έχει μικρή απόκλιση Kullback-Leibler από την άγνωστη κατανομή από την οποία προκύπτουν τα δειγματικά σημεία $\\mathbf{x}_v$. Έτσι, η RBM επιτρέπει στην ουσία τη δημιουργία μιας κατανομής η οποία να προσεγγίζει όσο το δυνατό περισσότερο την άγνωστη κατανομή. Στη διαδικασία αυτή, η μη διασύνδεση μεταξύ νευρώνων του ίδιου επιπέδου επιταχύνει την παραγωγή των επιθυμητών στοιχείων και αποτελεί την αφορμή για την επιβολή του επιπλέον περιορισμού που διαχωρίζει τις RBM από τις υπόλοιπες Μηχανές Boltzmann.</font>-->\n",
    "\n",
    "- <font color='#486393'>Τα Deep Belief Networks (DBNs) αποτελούν στη βάση τους μια σύνθεση απλούστερων δικτύων, όπως είναι για παράδειγμα οι RBMs που συζητώνται στο παρόν Notebook, ή δομές όπως οι autoencoders (οι οποίοι είναι και ο λόγος για τον οποίο οι RBMs δε χρησιμοποιούνται πλέον συχνά). Η αρχιτεκτονική τους είναι τέτοια, ώστε το κρυφό επίπεδο κάθε απλούστερης δομής να αποτελεί το ορατό επίπεδο της επόμενης.</font>\n",
    "\n",
    "<font color='#486393'>Χρησιμοποιούνται σε προβλήματα που χρησιμοποιούνται και οι RBMs, τα οποία αφορούν κυρίως την εξαγωγή χαρακτηριστικών από δεδομένα μέσω μη επιβλεπόμενης μάθησης, αφού επιτρέπουν την αναπαράστασή τους από μια αρχική μορφή σε μορφές υψηλότερου επιπέδου (higher level representations). Φυσικά, μπορούν να εκπαιδευτούν και περαιτέρω μέσω επίβλεψης και να αξιοποιηθούν σε προβλήματα ταξινόμησης, ή να αποτελέσουν το πρώτο κομμάτι μιας βαθύτερης αρχιτεκτονικής.</font>\n",
    "\n",
    "<font color='#486393'>Αυτό που διαφοροποιεί τα DBNs από τις RBMs είναι πως είναι πιο αποδοτικά στην επίλυση των προβλημάτων αυτών, καθώς το αυξημένο τους βάθος επιδρά στην απόδοσή τους με τον τρόπο που επιδρά και στα γνωστά βαθιά δίκτυα, επιτρέποντάς τους δηλαδή να εκπαιδευτούν αποτυπώνοντας πιο σύνθετες αλληλεπιδράσεις. Μάλιστα, ένα σημαντικό τους προτέρημα είναι πως η δυνατότητά τους να αφομοιώνουν πολυπλοκότερες δομές δε μεταφράζεται σε αυξημένη πολυπλοκότητα στον τρόπο εκπαίδευσής τους. Αντιθέτως, μπορούν να εκπαιδευτούν «greedily», δηλαδή εφαρμόζοντας αλγορίθμους εκπαίδευσης όπως ο CD σε κάθε απλούστερο δίκτυο ξεχωριστά και θεωρώντας την έξοδο στην οποία συγκλίνει ως είσοδο του επόμενου.</font>\n",
    "\n",
    "- <font color='#486393'>Πέραν της εξαγωγής χαρακτηριστικών και της ταξινόμησης, οι οποίες αναφέρθηκαν παραπάνω, ορισμένες επιπλέον εφαρμογές όπου αξιοποιούνται τα DBNs και οι RBMs είναι οι ακόλουθες:</font>\n",
    "\n",
    "<font color='#486393'>[1] Διαστατική μείωση, επιλέγοντας εξόδους μικρότερου μεγέθους σε σχέση με την είσοδο, δηλαδή το σύνολο των δειγματικών σημείων.</font>\n",
    "\n",
    "<font color='#486393'>[2] Συστήματα σύστασης (Recommender systems), όπου δεδομένων ορισμένων δεδομένων εισόδου, π.χ. ταινίες, αναγνωρίζουν κάποια κρυφά χαρακτηριστικά όπως το είδος ταινίων που προτιμάται συχνότερα και βάσει αυτού επιστρέφουν άλλες ταινίες αντίστοιχου είδους.</font>\n",
    "\n",
    "<font color='#486393'>[3] Θεματική μοντελοποίηση (topic modelling), κυρίως στην περιοχή της επεξεργασίας φυσικής γλώσσας, η οποία εμφανίζει συγγένειες με την έννοια της συσταδοποίησης δεδομένων.</font>\n",
    "\n",
    "<font color='#486393'>[4] Ανακατασκευή εικόνας (image reconstruction), όπου θολές εικόνες ενισχύονται ή κρυφά σημεία σε φωτογραφίες (π.χ. το πίσω μέρος ενός σπιτιού για το οποίο φαίνεται μόνο η πρόσοψη) ανακατασκευάζονται.</font>\n",
    "\n",
    "<font color='#486393'>[5] Παραγωγή βίντεο, αξιοποιώντας τη δυνατότητα δειγματοληψίας για διαφορετικά καρέ (frames).</font>\n",
    "\n",
    "<font color='#486393'>[6] Στη Φυσική, για τη μελέτη κβαντικών συστημάτων όπου μετρήσεις στις παρατηρούμενες καταστάσεις τους μπορούν να χρησιμοποιηθούν για την ανακατασκευή της κυματοσυνάρτησης που τα περιγράφει.</font>\n",
    "\n",
    "<font color='#486393'>[7] Στην Ιατρική, σε εγκεφαλογραφήματα (βλ. [εδώ](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5967386/)).</font>\n",
    "\n",
    "<font color='#486393'>Σε κάθε περίπτωση, πρέπει να έχει κανείς υπ' όψιν πως τα RBMs και τα DBNs που κατασκευάζονται από αυτά έχουν αντικατασταθεί για τις εφαρμογές αυτές από συστήματα όπως τα GANs (Generative Adversarial Networks) ή οι προαναφερθέντες autoencoders.\n",
    "\n",
    "- <font color='#486393'>Όπως αναφέρθηκε και στην απάντηση του δεύτερου ερωτήματος, οι RBMs δεν έχουν το μονοπώλιο στην κατασκευή DBNs. Άλλες δομές που μπορούν να αξιοποιηθούν για την κατασκευή DBNs είναι οι autoencoders (βλ. [εδώ](https://ieeexplore.ieee.org/abstract/document/7882660)), τα κρυφά Μαρκοβιανά μοντέλα (Hidden Markov Models - HMMs - βλ. [εδώ](https://ieeexplore.ieee.org/document/6707732)), ή τα GANs (Generative Adversarial Networks - βλ. [εδώ](https://arxiv.org/abs/1909.06134)). </font>\n",
    "\n",
    "- <font color='#486393'>Σε όλα τα αποτελέσματα που φαίνονται στο classification report του κώδικα, η ακρίβεια (Accuracy) είναι πλήρως εναρμονισμένη με τις macro- και micro-averaged εκδοχές των Precision, Recall και F1-Score, επομένως τα μοντέλα δεν έχουν συγκεκριμένες προτιμήσεις σε τύπους λαθών και άρα η ακρίβεια αυτή καθ' αυτή επαρκεί για την αξιολόγηση των μοντέλων. Βάσει αυτών, τα αποτελέσματα για κάθε μοντέλο συνοψίζονται στον ακόλουθο πίνακα.</font>\n",
    "\n",
    "| Μοντέλο | Ακρίβεια |\n",
    "| :---: | :---: |\n",
    "| Λογιστική παλινδρόμηση χωρίς RBM | **92%** |\n",
    "| Λογιστική παλινδρόμηση με RBM | **97%** |\n",
    "| Λογιστική παλινδρόμηση με DBN από 2 RBM | **98%** |\n",
    "\n",
    "<font color='#486393'>Επειδή τα μοντέλα εκπαιδεύτηκαν για πολλές εποχές, σε πολύ μεγάλο όγκο δεδομένων, τα οποία έχουν μια σχετική απλότητα (αποτελούν το «Hello world» πρόβλημα για την αναγνώριση εικόνας), όλα τους εμφανίζουν σχετικά καλή απόδοση. Παρ' όλα αυτά, η επίδραση της εξαγωγής χαρακτηριστικών των RBMs είναι εμφανής, αφού ένα RBM στην είσοδο του μοντέλου λογιστικής παλινδρόμησης οδηγεί σε αύξηση της απόδοσης σε ακόμα υψηλότερα ποσοστά. Σημειώνεται πως, παρότι η αύξηση αυτή είναι μόλις +5%, έχει σημασία το γεγονός ότι αποτελεί αύξηση σε ένα ήδη υψηλό ποσοστό (92%). Σε ό,τι αφορά το τρίτο μοντέλο, τα αποτελέσματα επίσης επαληθεύουν την ανάλυση που πραγματοποιήθηκε στα προηγούμενα ερωτήματα, αφού το τελικό μοντέλο εμφανίζει τη μέγιστη ακρίβεια, επιτυγχάνοντας να αυξήσει την ήδη πολύ υψηλή ακρίβεια του μοντέλου λογιστικής παλινδρόμησης με 1 RBM στην είσοδό του (97%). Σίγουρα, η επίδραση που έχει η προσθήκη του/των RBM(s) στο συνολικό χρόνο εκτέλεσης δεν είναι αμελητέα, όμως η αύξηση της συνολικής ακρίβειας στην οποία οδηγούν σίγουρα έχει πολύ υψηλότερη βαρύτητα.</font>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab5_RBM_DBN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
