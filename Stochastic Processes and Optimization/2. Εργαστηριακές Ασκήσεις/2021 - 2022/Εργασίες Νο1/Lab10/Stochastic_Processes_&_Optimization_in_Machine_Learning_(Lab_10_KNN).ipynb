{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Sc2hbFf_5x4"
   },
   "source": [
    "# Αλγόριθμος ΚΝΝ\n",
    "\n",
    "Στο παρακάτω παράδειγμα θα εξετάσουμε πως λειτουργεί ο αλγόριθμος Κ-Nearest Neighbors ([KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm?msclkid=0c75f966cf9c11ecbab5cba311a90428)), χρησιμοποιώντας το [iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set?msclkid=33057869cf9c11ec8e488a45734cbc68) σε προβλήματα κατηγοριοποίησης και προβλέψεων. Το αρχείο που θα χρησιμοποιήσετε είναι διαθέσιμο [**εδώ**](https://github.com/nkostopoulos/StochasticsLabPublic/blob/master/lab10/KNN/iris.csv).\n",
    "\n",
    "\n",
    "### Ερωτήσεις\n",
    "\n",
    "\n",
    "\n",
    "*   Να περιγράψετε συνοπτικά το τρόπο λειτουργίας του αλγορίθμου ΚΝΝ\n",
    "*   Ποια είναι τα πλεονεκτήματα και τα μειονεκτήματα του συγκεκριμένου αλγορίθμου; \n",
    "*   Ο συγκεκριμένος αλγόριθμος είναι αποδοτικός στην περίπτωση που έχουμε μεγάλο πλήθος μεταβλητών; Τι συμβαίνει στις περιπτώσεις όπου επιλέξουμε μεγάλο ή μικρό k; \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Απαντήσεις\n",
    "\n",
    "- <font color='#486393'>Η βασική ιδέα πίσω από τον αλγόριθμο k-Nearest Neighbours (kNN) είναι πως δύο σημεία $\\mathbf{x}_A = \\left[x_A^1, x_A^2, \\dots, x_A^m\\right]^\\top$ και $\\mathbf{x}_B = \\left[x_B^1, x_B^2, \\dots, x_B^m\\right]^\\top$ ενός $m$-διάστατου χώρου αναμένεται να έχουν όμοιες ιδιότητες, εφόσον η απόστασή τους στο χώρο αυτό είναι μικρή. Με τον όρο απόσταση δεν αναφέρεται κανείς αυστηρά σε Ευκλείδεια απόσταση, αλλά σε οποιοδήποτε καλώς ορισμένο μέτρο απόστασης (π.χ. Manhattan, Mahalanobis, κτλ).</font>\n",
    "\n",
    "<font color='#486393'>Έτσι, στα πλαίσια προβλημάτων ταξινόμησης, ο αλγόριθμος εκπαιδεύεται καταχωρώντας ένα σύνολο από σημεία και τις αντίστοιχες κατηγορίες όπου αυτά ανήκουν. Κατόπιν, η ταξινόμηση νέων σημείων σε κάποια από τις υπάρχουσες κατηγορίες (φάση εκτέλεσης του αλγορίθμου) γίνεται ως εξής: αφότου υπολογιστεί η απόσταση (ανάλογα με το πώς αυτή έχει οριστεί) του νέου σημείου από όλα τα σημεία του δείγματος εκπαίδευσης, εντοπίζονται τα $k$ σημεία εκπαίδευσης τα οποία βρίσκονται πιο κοντά στο νέο σημείο (δηλαδή εντοπίζονται οι $k$ πρώτοι γείτονές του). Κατόπιν, ελέγχεται η κατηγορία στην οποία ανήκουν τα σημεία αυτά και η ταξινόμηση του νέου σημείου γίνεται στην κατηγορία όπου ανήκει η πλειοψηφία των $k$ υπό μελέτη σημείων του δείγματος εκπαίδευσης.</font>\n",
    "\n",
    "- <font color='#486393'>Το μεγαλύτερο προτέρημα του αλγορίθμου kNN έγκειται στην απλότητά του - δεν είναι εξάλλου τυχαίο πως είναι από τους πρώτους αλγορίθμους επιβλεπόμενης μάθησης που διδάσκεται συνήθως. Πρόσθετα, δεν υιοθετεί οποιαδήποτε παραδοχή, σε αντίθεση με άλλους αλγορίθμους (όπως για παράδειγμα ο Naive Bayes, ο οποίος είναι το αντικείμενο μελέτης του άλλου notebook της παρούσας εργαστηριακής άσκησης). Επιπλέον, μπορεί να αξιοποιηθεί τόσο για προβλήματα ταξινόμησης (classification), όσο και για προβλήματα παλινδρόμησης (regression). Τέλος, δεν απαιτεί την εκπαίδευση ενός σύνθετου μοντέλου με υψηλό πλήθος υπερπαραμέτρων οι οποίες πρέπει να ρυθμιστούν, ή παραμέτρων οι οποίες πρέπει να εκπαιδευτούν (όπως π.χ. στα νευρωνικά δίκτυα). Η μοναδική υπερπαράμετρός του είναι η τιμή του $k$.</font>\n",
    "\n",
    "<font color='#486393'>Σε ό,τι αφορά τα μειονεκτήματα του αλγορίθμου, το σημαντικότερο εξ αυτών είναι η ταχύτητά του: είναι ένας αλγόριθμος υψηλής υπολογιστικής πολυπλοκότητας, η οποία αυξάνεται δραστικά καθώς αυξάνεται το μέγεθος του δειγματικού συνόλου (είσοδος) ή το πλήθος των διαφορετικών κατηγοριών (έξοδος) στα πλαίσια ενός προβλήματος ταξινόμησης. Ένα άλλο μειονέκτημά του είναι πως υποφέρει από τη γνωστή <i>κατάρα της διαστατικότητας</i> (dimensionality curse), δηλαδή δεν αποδίδει εξίσου καλά όταν το πλήθος των χαρακτηριστικών κάθε δειγματικού σημείου αυξάνεται σημαντικά. Επιπροσθέτως, ο kNN είναι πολύ επιρρεπής σε outliers και σημεία επιρροής, αφού δεν έχει κάποιο τρόπο να φιλτράρει τη συνεισφορά τέτοιων σημείων κατά τις προβλέψεις του. Στα ίδια πλαίσια, εμφανίζει την εγγενή αδυναμία να διαχειριστεί απουσιάζουσες τιμές από το δειγματικό σύνολο, συνεπώς απαιτεί σημαντική προεπεξεργασία των δεδομένων πριν την εκτέλεσή του. Αξίζει επιπλέον να αναφερθεί η αδυναμία του kNN να κάνει προβλέψεις εφόσον έχει εκπαιδευτεί σε μη-ισορροπημένα σύνολα εκπαίδευσης (imbalanced datasets), αφού θα υπάρχει ισχυρή προτίμηση προς τη συχνότερη κατηγορία. Τέλος, η ρύθμιση της μοναδικής υπερπαραμέτρου του, $k$, δεν είναι τετριμμένη διαδικασία και μάλιστα, λανθασμένες επιλογές για αυτήν μπορεί να οδηγήσουν σε ένα μοντέλο με χαμηλή προβλεπτική ικανότητα ή/και κακή γενικευσιμότητα.</font>\n",
    "\n",
    "- <font color='#486393'>Όπως αναφέρθηκε και στο προηγούμενο ερώτημα σχετικά με τα μειονεκτήματα του kNN, ένα μεγάλο πλήθος μεταβλητών επηρεάζει αρνητικά την προβλεπτική ικανότητά του, λόγω της κατάρας της διαστατικότητας. Σε ό,τι έχει να κάνει με την επιλογή του $k$, όπως επίσης αναλύθηκε προηγουμένως, η σημασία της είναι πολύ υψηλή. Για πολύ μικρές τιμές του $k$, η ταξινόμηση ενός νέου δειγματικού σημείου βασίζεται σε πολύ περιορισμένο πλήθος πρώτων γειτόνων. Ως εκ τούτου, η προαναφερθείσα επιρροή από outliers ή άλλων σημείων επιρροής γίνεται ακόμα σημαντικότερη. Από την άλλη, μεγάλες τιμές του $k$ αυξάνουν σημαντικά την υπολογιστική πολυπλοκότητα του αλγορίθμου, χωρίς όμως να εγγυώνται ένα ακριβέστερο αποτέλεσμα, αφού μια τέτοια πρακτική πάει κόντρα στη φιλοσοφία του kNN, ότι δηλαδή σημεία τα οποία απέχουν μικρή απόσταση είναι αυτά που αναμένεται να έχουν όμοιες ιδιότητες.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YdDd567UDrAH"
   },
   "outputs": [],
   "source": [
    "# Make Predictions with k-nearest neighbors on the Iris Flowers Dataset\n",
    "from csv import reader\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Z0S9ZQ57Dv8-"
   },
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "        print('[%s] => %d' % (value, i))\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        value_min = min(col_values)\n",
    "        value_max = max(col_values)\n",
    "        minmax.append([value_min, value_max])\n",
    "    return minmax\n",
    "\n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2APEXH9DD3S_"
   },
   "source": [
    "\n",
    "\n",
    "*   Αναφέρετε άλλες μεθόδους υπολογισμού της απόστασης\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#486393'>Στα παρακάτω, ως απόσταση ορίζεται η Ευκλείδεια απόσταση (δηλ. η $L_{2}$-νόρμα), παρ' όλα αυτά οι επιλογές δεν περιορίζονται σε αυτήν. Άλλες αποστάσεις είναι η απόσταση Manhattan (δηλ. η $L_{1}$-νόρμα), η οποία για δύο διανύσματα $\\mathbf{x}_A = \\left[x_A^1, x_A^2, \\dots, x_A^m\\right]^\\top$ και $\\mathbf{x}_B = \\left[x_B^1, x_B^2, \\dots, x_B^m\\right]^\\top$ ορίζεται ως</font>\n",
    "\n",
    "<font color='#486393'>$$ L_1\\left(\\mathbf{x}_A,\\mathbf{x}_B\\right) = \\sum_{k=1}^{m}|x_A^k-x_B^k|, $$</font>\n",
    "\n",
    "<font color='#486393'>η $L_{\\infty}$-νόρμα, δηλαδή η ποσότητα</font>\n",
    "\n",
    "<font color='#486393'>$$ L_\\infty\\left(\\mathbf{x}_A,\\mathbf{x}_B\\right) = \\text{max}_k{|x_A^k-x_B^k|}, $$</font>\n",
    "\n",
    "<font color='#486393'>κ.ο.κ. Σημειώνεται εδώ πως εάν το ερώτημα δεν αφορά άλλα είδη απόστασης, αλλά άλλες μεθόδους υπολογισμού της Ευκλείδειας απόστασης στην Python, μια εναλλακτική στον κώδικα που έχει δοθεί παρακάτω είναι η ακόλουθη συνάρτηση, η οποία εκμεταλλεύεται το vectorization που επιτρέπει η βιβλιοθήκη numpy.</font>\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "def euclidean_distance_alt(row1, row2):\n",
    "    row1, row2 = np.array(row1), np.array(row2)\n",
    "    return np.sqrt((row1-row2)**2.sum())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "R4OjL9p18HL-"
   },
   "outputs": [],
   "source": [
    "# Calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    # Στο ακόλουθο, το αρχικό range ήταν το range(len(row1)-1)\n",
    "    # Αυτό διορθώθηκε, καθώς έδινε λάθος αποτελέσματα όταν καλούταν\n",
    "    # για να ταξινομήσει ένα νέο δεδομένο (π.χ. υπολόγιζε την απόσταση\n",
    "    # ανάμεσα στα [1, 0], [1, 1] ως μηδενική)\n",
    "    # Προκειμένου να αγνοείται στο training το τελευταίο entry της γραμμής\n",
    "    # άλλαξε η καταχώρηση του train_row στην get_neighbors\n",
    "    # από train_row σε train_row[:-1]\n",
    "    for i in range(len(row1)):\n",
    "        distance += (row1[i] - row2[i])**2\n",
    "    return sqrt(distance)\n",
    "\n",
    "# Locate the most similar neighbors\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "    distances = list()\n",
    "    for train_row in train:\n",
    "        dist = euclidean_distance(test_row, train_row[:-1])\n",
    "        distances.append((train_row, dist))\n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    neighbors = list()\n",
    "    for i in range(num_neighbors):\n",
    "        neighbors.append(distances[i][0])\n",
    "    return neighbors\n",
    "\n",
    "# Make a prediction with neighbors\n",
    "def predict_classification(train, test_row, num_neighbors):\n",
    "    neighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "    output_values = [row[-1] for row in neighbors]\n",
    "    prediction = max(set(output_values), key=output_values.count)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIVzwwpSEeE-"
   },
   "source": [
    "\n",
    "\n",
    "*   Δοκιμάστε με τα παρακάτω ζεύγη τιμών και σημειώστε τα αποτελέσματα\n",
    "\n",
    "\n",
    "> *   [4.9, 3.1, 1.5, 0.1]\n",
    ">*   [6.9, 3.1, 4.9, 1.5]\n",
    ">*   [5.0, 2.0, 3.5, 1.0]\n",
    ">*   [5.6, 2.7, 4.2, 1.3]\n",
    ">*   [6.3, 3.3, 6.0, 2.5]\n",
    ">*   [5.7, 2.9, 4.2, 1.3]\n",
    ">*   [5.9, 3.0, 5.1, 1.8]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vVci7H7jExef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iris-virginica] => 0\n",
      "[Iris-setosa] => 1\n",
      "[Iris-versicolor] => 2\n",
      "Data=[4.5, 2.3, 1.3, 0.3], Predicted: 1\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction with KNN on Iris Dataset\n",
    "filename = 'iris.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# define model parameter\n",
    "num_neighbors = 5\n",
    "# define a new record\n",
    "row = [4.5, 2.3, 1.3, 0.3]\n",
    "# predict the label\n",
    "label = predict_classification(dataset, row, num_neighbors)\n",
    "print('Data=%s, Predicted: %s' % (row, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#486393'>Το ακόλουθο κελί περιλαμβάνει τον κώδικα βάσει του οποίου ταξινομούνται τα δοθέντα διανύσματα χαρακτηριστικών στα πλαίσια του iris dataset. Όπως και στο παράδειγμα, χρησιμοποιείται $k = 5$.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data=[4.9, 3.1, 1.5, 0.1], Predicted: 1\n",
      "Data=[6.9, 3.1, 4.9, 1.5], Predicted: 2\n",
      "Data=[5.0, 2.0, 3.5, 1.0], Predicted: 2\n",
      "Data=[5.6, 2.7, 4.2, 1.3], Predicted: 2\n",
      "Data=[6.3, 3.3, 6.0, 2.5], Predicted: 0\n",
      "Data=[5.7, 2.9, 4.2, 1.3], Predicted: 2\n",
      "Data=[5.9, 3.0, 5.1, 1.8], Predicted: 0\n"
     ]
    }
   ],
   "source": [
    "test_rows = [[4.9, 3.1, 1.5, 0.1],\n",
    "             [6.9, 3.1, 4.9, 1.5],\n",
    "             [5.0, 2.0, 3.5, 1.0],\n",
    "             [5.6, 2.7, 4.2, 1.3],\n",
    "             [6.3, 3.3, 6.0, 2.5],\n",
    "             [5.7, 2.9, 4.2, 1.3],\n",
    "             [5.9, 3.0, 5.1, 1.8]]\n",
    "\n",
    "for row in test_rows:\n",
    "    label = predict_classification(dataset, row, num_neighbors)\n",
    "    print('Data=%s, Predicted: %s' % (row, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#486393'>Με βάση αυτά, προκύπτουν τα αποτελέσματα που απεικονίζονται στον ακόλουθο πίνακα.</font>\n",
    "\n",
    "| Διάνυσμα | Πρόβλεψη | Ταξινόμηση |\n",
    "| :---: | :---: | :---: |\n",
    "| [4.9, 3.1, 1.5, 0.1] | 1 | Iris-setosa |\n",
    "| [6.9, 3.1, 4.9, 1.5] | 2 | Iris-versicolor |\n",
    "| [5.0, 2.0, 3.5, 1.0] | 2 | Iris-versicolor |\n",
    "| [5.6, 2.7, 4.2, 1.3] | 2 | Iris-versicolor |\n",
    "| [6.3, 3.3, 6.0, 2.5] | 0 | Iris-virginica |\n",
    "| [5.7, 2.9, 4.2, 1.3] | 2 | Iris-versicolor |\n",
    "| [5.9, 3.0, 5.1, 1.8] | 0 | Iris-virginica |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9__nuQqGGyN4"
   },
   "source": [
    "\n",
    "\n",
    "*   Δοκιμάστε να υπολογίσετε την απόσταση με τη μετρική [Manhattan](https://iq.opengenus.org/manhattan-distance/#:~:text=Manhattan%20distance%20is%20a%20distance%20metric%20between%20two,the%20measures%20in%20all%20dimensions%20of%20two%20points.?msclkid=50bbf70ecfa011ec91862d6b9263d761) για τα ζέυγη που σας έχουν δοθεί. Παρατηρείτε κάποια διαφοροποίηση ως προς τα αποτελέσματα;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#486393'>Επανορίζοντας ορισμένες συναρτήσεις προκειμένου η παραπάνω διαδικασία να πραγματοποιείται μέσω της απόστασης Manhattan, προκύπτουν τα ακόλουθα.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data=[4.9, 3.1, 1.5, 0.1], Predicted: 1\n",
      "Data=[6.9, 3.1, 4.9, 1.5], Predicted: 2\n",
      "Data=[5.0, 2.0, 3.5, 1.0], Predicted: 2\n",
      "Data=[5.6, 2.7, 4.2, 1.3], Predicted: 2\n",
      "Data=[6.3, 3.3, 6.0, 2.5], Predicted: 0\n",
      "Data=[5.7, 2.9, 4.2, 1.3], Predicted: 2\n",
      "Data=[5.9, 3.0, 5.1, 1.8], Predicted: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate the Manhattan distance between two vectors\n",
    "def manhattan_distance(row1, row2):\n",
    "    row1, row2 = np.array(row1), np.array(row2)\n",
    "    return (abs(row1-row2)).sum()\n",
    "\n",
    "def get_neighbors_new(train, test_row, num_neighbors):\n",
    "    distances = list()\n",
    "    for train_row in train:\n",
    "        dist = manhattan_distance(test_row, train_row[:-1])\n",
    "        distances.append((train_row, dist))\n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    neighbors = list()\n",
    "    for i in range(num_neighbors):\n",
    "        neighbors.append(distances[i][0])\n",
    "    return neighbors\n",
    "\n",
    "def predict_classification_new(train, test_row, num_neighbors):\n",
    "    neighbors = get_neighbors_new(train, test_row, num_neighbors)\n",
    "    output_values = [row[-1] for row in neighbors]\n",
    "    prediction = max(set(output_values), key=output_values.count)\n",
    "    return prediction\n",
    "\n",
    "for row in test_rows:\n",
    "    label = predict_classification_new(dataset, row, num_neighbors)\n",
    "    print('Data=%s, Predicted: %s' % (row, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#486393'>Καθίσταται εμφανές πως στο συγκεκριμένο παράδειγμα η αλλαγή της μετρικής από Ευκλείδεια σε Manhattan δεν οδήγησε σε ποιοτικά διαφορετικά αποτελέσματα σε ό,τι έχει να κάνει με την ταξινόμηση των νέο δειγματικών σημείων.</font>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Stochastic_Processes_&_Optimization_in_Machine_Learning_(Lab_10_KNN).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
