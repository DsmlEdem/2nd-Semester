{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XLZdEbAy2jfr"
   },
   "source": [
    "<h1><b>Markov Decision Processes</h1></b>\n",
    "<p align=\"justify\">Στη συγκεκριμένη άσκηση θα μελετήσετε τους αλγορίθμους <i>Policy Iteration</i> και <i>Value Iteration</i>, καθώς και θα εξοικειωθείτε με βασικές έννοιες των <i>Markov Decision Processes</i>. Οι αλγόριθμοι <i>Policy Iteration</i> και <i>Value Iteration</i> είναι από τους βασικούς αλγορίθμους δυναμικού προγραμματισμού που χρησιμοποιούνται για την επίλυση της εξίσωσης <i>Bellman</i> σε <i>Markov Decision Processes</i>.</p> \n",
    "<p align=\"justify\">Το πρόβλημα που θα μελετήσετε είναι αυτό της παγωμένης λίμνης (Frozen Lake) με μέγεθος πλέγματος 8 x 8.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6VsUV229__zO"
   },
   "source": [
    "<h2><b>Εξοικείωση με τη βιβλιοθήκη <i>Gym</i></b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OM8ivgOJAg_H"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_puV3ugeAnbU"
   },
   "source": [
    "Με την παρακάτω εντολή, ορίζετε το πρόβλημα που θα μελετηθεί:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ep-MvIUCAxT8"
   },
   "outputs": [],
   "source": [
    "env_name = 'FrozenLake8x8-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBKBXJDUBRUh"
   },
   "source": [
    "<font color='#486393'>Οι απαντήσεις στα ερωτήματα της εργασίας θα γράφονται με αυτό το χρώμα, προκειμένου να διαφοροποιούνται από την εκφώνηση και τις οδηγίες της. Σημειώνεται, αρχικά, πως το περιβάλλον `'FrozenLake8x8-v0'` δεν έχει υποστήριξη στην τρέχουσα έκδοση της βιβλιοθήκης `gym`. Για τους σκοπούς της εργασίας εγκαταστάθηκε η έκδοση `0.18.3`.</font>\n",
    "\n",
    "Με τις παρακάτω εντολές, θα επαναφέρετε τον Agent στην αρχική του θέση και θα οπτικοποιήσετε το πλέγμα και τη θέση του Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6lqbG9zBgdi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FX2res4JBlYb"
   },
   "source": [
    "Με τις παρακάτω εντολές, ορίζετε την επόμενη ενέργεια με τυχαίο τρόπο και ο Agent κάνει ένα βήμα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gq7q944YBx0Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "next_action = env.action_space.sample()\n",
    "env.step(next_action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mV4A7lsLB54y"
   },
   "source": [
    "Να εκτελέσετε αρκετές φορές τις τελευταίες εντολές και να παρατηρήσετε κάθε φορά την ενέργεια που ζητείται από τον agent να εκτελέσει και την ενέργεια που αυτός πραγματοποιεί. Πραγματοποιεί πάντοτε ο agent την κίνηση που του ζητείται; Είναι ντετερμινιστικές ή στοχαστικές οι κινήσεις του agent;\n",
    "\n",
    "<font color='#486393'>Εκτελώντας το κελί που αφορά τις δράσεις του agent αρκετές φορές καθίσταται εμφανές πως οι κινήσεις του είναι στοχαστικές και όχι ντετερμινιστικές. Με άλλα λόγια, η κίνησή του σε κάθε βήμα προς κάθε κατεύθυνση χαρακτηρίζεται από κάποια πιθανότητα πραγματοποίησης, η οποία δεν είναι (εν γένει) ίση με τη μονάδα. Αυτό τεκμαίρεται από την παρατήρηση πως αρκετά συχνά η κίνηση που ζητείται από τον agent να εκτελέσει διαφέρει από αυτήν που εν τέλει αυτός εκτελεί.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAL4we3gDV_J"
   },
   "source": [
    "<h2><b>Ερωτήσεις</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQKm4VAUChi1"
   },
   "source": [
    "<ul>\n",
    "<li>Μελετώντας <a href=\"https://gym.openai.com/envs/FrozenLake-v0/\">αυτό</a> και βασισμένοι στα συμπεράσματα του προηγούμενου ερωτήματος να περιγράψετε σύντομα το πρόβλημα της παγωμένης λίμνης ως πρόβλημα βελτιστοποίησης. Ποιος είναι ο στόχος του agent;</li>\n",
    "<li>Να διατυπώσετε την ιδιότητα <i>Markov</i>. Πώς απλοποιεί η ιδιότητα αυτή τη μελέτη του συγκεκριμένου προβλήματος;</li>\n",
    "<li>Να περιγράψετε σύντομα τους αλγορίθμους <i>Policy Iteration</i> και <i>Value Iteration</i>, δίνοντας έμφαση στο διαφορετικό τρόπο με τον οποίο προσεγγίζουν την επίλυση του προβλήματος. Είναι εγγυημένο ότι οι δύο αλγόριθμοι θα συγκλίνουν στη βέλτιστη πολιτική; Αν ναι, οδηγούν σε ίδια ή διαφορετική βέλτιστη πολιτική;</li>\n",
    "<li>Να εκτελέσετε τα προγράμματα που σας δίνονται, τα οποία επιλύουν το\n",
    "πρόβλημα της παγωμένης λίμνης, χρησιμοποιώντας τους αλγορίθμους <i>Policy\n",
    "Iteration</i> και <i>Value Iteration</i> αντίστοιχα. Ποια μέθοδος συγκλίνει στη βέλτιστη λύση σε λιγότερα βήματα; Τι συμπέρασμα βγάζετε; Να ελέγξετε το χρόνο εκτέλεσης του κάθε προγράμματος, χρησιμοποιώντας την εντολή <i>time</i>. Τι συμπέρασμα βγάζετε ως προς την πολυπλοκότητα του κάθε αλγορίθμου;</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S6mci5P4HJ_1"
   },
   "source": [
    "<h2><b>Policy Iteration</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_43MjfJ9G8v7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy-Iteration converged at step 8.\n",
      "Average scores =  1.0\n",
      "Wall time: 1.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Solving FrozenLake8x8 environment using Policy iteration.\n",
    "Author : Moustafa Alzantot (malzantot@ucla.edu)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "\n",
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Runs an episode and return the total reward \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0, n = 100):\n",
    "    scores = [run_episode(env, policy, gamma, False) for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            q_sa[a] = sum([p * (r + gamma * v[s_]) for p, s_, r, _ in  env.P[s][a]])\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "def compute_policy_v(env, policy, gamma=1.0):\n",
    "    \"\"\" Iteratively evaluate the value-function under policy.\n",
    "    Alternatively, we could formulate a set of linear equations in iterms of v[s] \n",
    "    and solve them to find the value function.\n",
    "    \"\"\"\n",
    "    v = np.zeros(env.nS)\n",
    "    eps = 1e-10\n",
    "    while True:\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            policy_a = policy[s]\n",
    "            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])\n",
    "        if (np.sum((np.fabs(prev_v - v))) <= eps):\n",
    "            # value converged\n",
    "            break\n",
    "    return v\n",
    "\n",
    "def policy_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Policy-Iteration algorithm \"\"\"\n",
    "    policy = np.random.choice(env.nA, size=(env.nS))  # initialize a random policy\n",
    "    max_iterations = 200000\n",
    "    gamma = 1.0\n",
    "    for i in range(max_iterations):\n",
    "        old_policy_v = compute_policy_v(env, policy, gamma)\n",
    "        new_policy = extract_policy(old_policy_v, gamma)\n",
    "        if (np.all(policy == new_policy)):\n",
    "            print ('Policy-Iteration converged at step %d.' %(i+1))\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_name  = 'FrozenLake8x8-v0'\n",
    "    env = gym.make(env_name)\n",
    "    env = env.unwrapped\n",
    "    optimal_policy = policy_iteration(env, gamma = 1.0)\n",
    "    scores = evaluate_policy(env, optimal_policy, gamma = 1.0)\n",
    "    print('Average scores = ', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcikBq6BHRQM"
   },
   "source": [
    "<h2><b>Value Iteration</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gHvcnTDcHGmH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  1.0\n",
      "Wall time: 1.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Solving FrozenLake8x8 environment using Value-Itertion.\n",
    "Author : Moustafa Alzantot (malzantot@ucla.edu)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "\n",
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
    "    total reward.\n",
    "    args:\n",
    "    env: gym environment.\n",
    "    policy: the policy to be used.\n",
    "    gamma: discount factor.\n",
    "    render: boolean to turn rendering on/off.\n",
    "    returns:\n",
    "    total reward: real value of the total reward recieved by agent under policy.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0,  n = 100):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "            run_episode(env, policy, gamma = gamma, render = False)\n",
    "            for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for next_sr in env.P[s][a]:\n",
    "                # next_sr is a tuple of (probability, next state, reward, done)\n",
    "                p, s_, r, _ = next_sr\n",
    "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def value_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Value-iteration algorithm \"\"\"\n",
    "    v = np.zeros(env.nS)  # initialize value-function\n",
    "    max_iterations = 100000\n",
    "    eps = 1e-20\n",
    "    for i in range(max_iterations):\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            q_sa = [sum([p*(r + prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.nA)] \n",
    "            v[s] = max(q_sa)\n",
    "        if (np.sum(np.fabs(prev_v - v)) <= eps):\n",
    "            print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
    "            break\n",
    "    return v\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_name  = 'FrozenLake8x8-v0'\n",
    "    gamma = 1.0\n",
    "    env = gym.make(env_name)\n",
    "    env = env.unwrapped\n",
    "    optimal_v = value_iteration(env, gamma);\n",
    "    policy = extract_policy(optimal_v, gamma)\n",
    "    policy_score = evaluate_policy(env, policy, gamma, n=1000)\n",
    "    print('Policy average score = ', policy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Απαντήσεις\n",
    "\n",
    "- <font color='#486393'>Το πρόβλημα Frozen Lake αποτελείται από ένα 8x8 πλέγμα, το οποίο αντιστοιχεί σε μια παγωμένη λίμνη. Κάθε κυψελίδα του πλέγματος μπορεί να αποτελεί είτε frozen surface (F), είτε hole (H), ενώ υπάρχουν και δύο ειδικές κυψελίδες, η starting point (S) και η goal (G). Επάνω στο πλέγμα μπορεί να κινείται ένας πράκτορας (agent), ο οποίος ξεκινά από την κυψελίδα S και στόχος του είναι να φτάσει στην κυψελίδα G, η οποία οδηγεί σε κάποια επιβράβευση (+1) για την επιτυχή «επίλυση» του προβλήματος, εκτελώντας ένα βήμα τη φορά.</font>\n",
    "    \n",
    "<font color='#486393'>Προκειμένου ο πράκτορας να μεταβεί στην κυψελίδα G πρέπει να διέρχεται από κυψελίδες τύπου F, διότι, εάν σε οποιοδήποτε βήμα μεταβεί σε κυψελίδα τύπου H, τότε το πρόβλημα τερματίζεται με τον πράκτορα ηττημένο και χωρίς επιβράβευση. Έτσι, το πρόβλημα πράγματι αντιστοιχεί σε ένα πρόβλημα βελτιστοποίησης, όπου ο πράκτορας καλείται από την αρχική του θέση να ακολουθήσει ένα μονοπάτι το οποίο να τον οδηγεί στην κυψελίδα G, λαμβάνοντας πάντα υπ' όψιν την προαναφερθείσα στοχαστική φύση της κίνησής του. Αυτή είναι, εξάλλου, και η έμπνευση για το όνομα του προβλήματος, αφού σε μια ολισθηρή επιφάνεια δεν είναι βέβαιο πως η επιθυμητή κίνηση ενός κινούμενου σώματος θα είναι και αυτή που εν τέλει αυτό θα πραγματοποιήσει.</font>\n",
    "\n",
    "- <font color='#486393'>Η ιδιότητα Markov σχετίζεται με διεργασίες τυχαίων μεταβλητών $\\left\\{X_n\\right\\}$ στις οποίες η πιθανότητα μετάβασης στην $k$-οστή κατάσταση, $X_k$, είναι ανεξάρτητη όλων των καταστάσεων που έχουν προηγηθεί **δεδομένης** της $X_{k-1}$. Μαθηματικά, αυτό εκφράζεται ως</font>\n",
    "\n",
    "<font color='#486393'>$$p\\left(X_k | X_{k-1}, X_{k-2}, X_{k-3}, \\dots, X_1\\right) = p\\left(X_k | X_{k-1}\\right)$$</font>\n",
    "\n",
    "<font color='#486393'>και συχνά διατυπώνεται εκλαϊκευτικά ως *το γεγονός πως το μέλλον καθορίζεται πλήρως δεδομένου μόνο του παρόντος και όχι του παρελθόντος*.</font>\n",
    "\n",
    "<font color='#486393'>Στο παρόν πρόβλημα, η υπόθεση ισχύος της ιδιότητας Markov απλοποιεί σημαντικά τη διαδικασία εκπαίδευσης του πράκτορα, καθώς αναγάγει το πρόβλημα στη μελέτη μιας πεπερασμένης Markov Decision Process (MDP). Έτσι, επιτρέπει τη μελέτη του μέσω τεχνικών δυναμικού προγραμματισμού, αφού το ευρύτερο πρόβλημα μπορεί να αναχθεί σε επιμέρους προβλήματα πολύ χαμηλότερης διάστασης που αφορούν κάθε φορά το αμέσως επόμενο βήμα του πράκτορα, δεδομένης της θέσης στην οποία κατέληξε από το αμέσως προηγούμενο.</font>\n",
    "\n",
    "- <font color='#486393'>Προτού περιγράψουμε τους δύο υπό μελέτη αλγόριθμους και εξετάσουμε τις διαφορές τους, είναι απαραίτητο να σημειωθεί ο σχετικός φορμαλισμός κάποιων θεμελιωδών ποσοτήτων του προβλήματος. Ακολουθώντας τους Sutton & Barto, η κατάσταση στην οποία βρίσκεται σε κάποιο βήμα, $t$, ο πράκτορας θα συμβολίζεται ως $S_t$, η προτεινόμενη κίνηση για το επόμενο βήμα ως $A_{t+1}$ ενώ η πιθανή επιβράβευση λόγω μετάβασης σε σχετική κυψελίδα ως $R_{t+1}$ (σημειώνεται πως στα πλαίσια του Frozen Lake v0, μόνο η κυψελίδα G οδηγεί σε μη μηδενική επιβράβευση, $r=1$).</font>\n",
    "\n",
    "<font color='#486393'>Σε ό,τι αφορά τον αλγόριθμο Policy Iteration, αυτός στην ουσία αποτελεί τη διαδοχική επανάληψη δύο επιμέρους διαδικασιών: της Policy Evaluation και της Policy Improvement, όπου η έννοια της «πολιτικής» (policy) αφορά εν γένει κάποια στρατηγική επίλυσης, η οποία στην προκείμενη περίπτωση ανάγεται στην εύρεση της βέλτιστης ακολουθίας κινήσεων επάνω στο πλέγμα από τη θέση S στη θέση G. Κατά την πρώτη διαδικασία (Policy Evaluation), ο αλγόριθμος υπολογίζει αναδρομικά τη value function κατά Bellman κάθε βήματος για μια τυχαία επιλεγμένη πολιτική, $\\pi$, η οποία στο δεδομένο πρόβλημα λαμβάνει τη μορφή</font>\n",
    "\n",
    "<font color='#486393'>$$ u_\\pi\\left(S_t\\right) = \\sum_{A_{t+1}}{\\pi\\left(A_{t+1}|S_{t}\\right)}\\sum_{S_{t+1},R_{t+1}}{p\\left(S_{t+1},R_{t+1}|S_t,A_{t+1}\\right)\\left[R_{t+1} + \\gamma u_\\pi\\left(S_{t+1}\\right)\\right]}, $$</font>\n",
    "\n",
    "<font color='#486393'>όπου $\\pi\\left(A_{t+1}|S_{t}\\right)$ είναι η πιθανότητα λήψης της απόφασης $A_{t+1}$ στην κατάσταση $S_t$, δεδομένης της πολιτικής $\\pi$, ενώ $\\gamma$ είναι ο κατά Bellman ρυθμός έκπτωσης (discount rate). Υπενθυμίζεται πως, εάν ο πράκτορας λάβει κάποια επιβράβευση $R_{t+m}$ στο βήμα $t +m$ αντί για το βήμα $t$, τότε αυτή θα συνδέεται με την επιβράβευση, $R_{t+1}$, εάν αυτή είχε ληφθεί στο βήμα $t$, μέσω της σχέσης $R_{t+m} = \\gamma^{m-1}R_{t+1}$. Προφανώς, η ύπαρξη και η μοναδικότητα της $u_\\pi$ εξασφαλίζεται μόνο όταν $\\gamma < 1$.</font>\n",
    "\n",
    "<font color='#486393'>Κατά τη δεύτερη διαδικασία (Policy Improvement), δεδομένων των $u_\\pi$, ελέγχεται το ενδεχόμενο η επιλογή μιας διαφορετικής πολιτικής, $\\pi^\\prime$, για την κατάσταση $S_t$ να οδηγεί σε μια υψηλότερη τιμή της value function. Ο έλεγχος αυτός ισοδυναμεί με την υπόθεση</font>\n",
    "\n",
    "<font color='#486393'>$$ q_\\pi\\left(S_t, A^\\prime_{t+1}\\right) \\geq u_\\pi\\left(S_t\\right),$$</font>\n",
    "\n",
    "<font color='#486393'>όπου $A^\\prime_{t+1} = \\pi^\\prime\\left(S_t\\right)$ και</font>\n",
    "\n",
    "<font color='#486393'>$$ q_\\pi\\left(S_t, A^\\prime_{t+1}\\right) = \\sum_{S_{t+1},R_{t+1}}{p\\left(S_{t+1},R_{t+1}|S_t,A^\\prime_{t+1}\\right)\\left[R_{t+1}+\\gamma u_\\pi\\left(S_{t+1}\\right)\\right]}. $$</font>\n",
    "    \n",
    "<font color='#486393'>Με την εύρεση μιας νέας πολιτικής η οποία να οδηγεί σε αύξηση της value function, η νέα τιμή της value function μπορεί να υπολογιστεί εκ νέου μέσω της διαδικασίας Policy Evaluation και κατόπιν να βελτιωθεί περαιτέρω μέσω της Policy Improvement. Έτσι, ο αλγόριθμος Policy Iteration κατ' ουσίαν επιτυγχάνει την εύρεση μιας βέλτιστης πολιτικής (η οποία οδηγεί σε βέλτιστη τιμή της value function), μέσω διαδοχικών επαναλήψεων των δύο διαδικασιών που αναλύθηκαν.</font>\n",
    "\n",
    "<font color='#486393'>Προχωρώντας στον αλγόριθμο Value Iteration, όπως υποδεικνύει και το όνομά του, η βασική του διαφορά από τον αλγόριθμο Policy Iteration είναι η ακόλουθη: αντί να ξεκινά από μια τυχαία πολιτική και με σταδιακές βελτιώσεις (κάτι το οποίο είναι υπολογιστικά κοστοβόρο) να καταλήγει στη βέλτιστη, ξεκινά από μια value function και σταδιακά τη βελτιώνει, μέχρι να καταλήξει στη μέγιστη τιμή της value function, η οποία με τη σειρά της αντιστοιχεί και σε βέλτιστη πολιτική. Ο τρόπος με τον οποίο το επιτυγχάνει αυτό είναι μέσω μιας επαναληπτικής διαδικασίας, η οποία περιγράφεται από τη σχέση</font>\n",
    "\n",
    "<font color='#486393'>$$ u_{k+1}\\left(S_t\\right) = \\text{max}_{A_{t+1}}\\sum_{S_{t+1},R_{t+1}}{p\\left(S_{t+1},R_{t+1}|S_t,A_{t+1}\\right)\\left[R_{t+1} + \\gamma u_k\\left(S_{t+1}\\right)\\right]}. $$</font>\n",
    "\n",
    "<font color='#486393'>Με άλλα λόγια, αντί να πραγματοποιεί την αλληλουχία Evaluation-Improvement για πολιτικές, ο αλγόριθμος Value Iteration βελτιώνει σε κάθε βήμα την τιμή της value function, μεγιστοποιώντας ως προς την αναμενόμενη επιβράβευση από το βήμα αυτό στο επόμενο. Η τελική τιμή (σύγκλιση) για την value function σε πρακτικές εφαρμογές αντιστοιχεί στην $u_K$, όπου $K$ είναι το βήμα κατά το οποίο η διαφορά $|u_K-u_{K-1}|$ γίνεται μικρότερη από ένα κατώφλι, $\\epsilon$, για πρώτη φορά.</font>\n",
    "\n",
    "<font color='#486393'>Κλείνοντας την παρουσίαση των αλγορίθμων, σε ό,τι αφορά τη σύγκλισή τους, αυτή είναι εξασφαλισμένη χάρη στην ιδιότητα Markov, η οποία καθιστά το πρόβλημα ισοδύναμο με μια **πεπερασμένη** MDP, όπως αναφέρθηκε στο προηγούμενο ερώτημα. Στην ουσία, το πρόβλημα ισοδυναμεί με τη διάταξη ενός πεπερασμένου πλήθους πραγματικών αριθμών (τις τιμές των value functions) στον πραγματικό άξονα, κάτι το οποίο είναι πάντοτε εφικτό. Εφόσον οι πολιτικές προκύπτουν από τις αντίστοιχες value functions και η σύγκλιση των αλγορίθμων επέρχεται όταν οι προκύπτουσες value functions ικανοποιούν τις εξισώσεις Bellman, και οι δύο αλγόριθμοι αναμένεται να συγκλίνουν σε βέλτιστη πολιτική. Ενδέχεται, φυσικά, να μην υπάρχει μία βέλτιστη πολιτική - δηλαδή μία μόνο πολιτική που να οδηγεί στη μέγιστη τιμή της value function -, γεγονός το οποίο σημαίνει πως εν γένει οι δύο αλγόριθμοι μπορεί να συγκλίνουν σε διαφορετική πολιτική.</font>\n",
    "\n",
    "- <font color='#486393'>Εκτελώντας τα δύο κελιά που αφορούν καθέναν από τους παραπάνω αλγορίθμους, καθίσταται εμφανές πως η μέθοδος Policy Iteration συγκλίνει στη βέλτιστη λύση σε πλήθος βημάτων $\\mathcal{O}\\left(10^0\\right)-\\mathcal{O}\\left(10^1\\right)$ (συγκεκριμένα 8), ενώ η μέθοδος Value Iteration συγκλίνει στη βέλτιστη λύση σε πλήθος βημάτων $\\mathcal{O}\\left(10^3\\right)$ (συγκεκριμένα 2357). Η διαφορά, δηλαδή, είναι περίπου 2 με 3 τάξεις μεγέθους, κάτι αναμενόμενο αφού το κατώφλι $\\epsilon$ που ελέγχει τη σύγκλιση του Value Iteration είναι πάρα πολύ μικρό (συγκεκριμένα είναι $\\epsilon = 10^{-20}$), ενώ εκ κατασκευής ο Policy Iteration συγκλίνει σε πολύ λιγότερα βήματα στη βέλτιστη λύση, αφού κάθε επανάληψή του εξασφαλίζει μια βελτιωμένη πολιτική.</font>\n",
    "\n",
    "<font color='#486393'>Παρ' όλα αυτά, φαίνεται επίσης πως η σύγκλιση των δύο αλγορίθμων επέρχεται σε χρόνους που ανήκουν στην ίδια κλίμακα. Συγκεκριμένα, το wall time (το οποίο βέβαια εξαρτάται από τη CPU του μηχανήματος στο οποίο τρέχει το Jupyter Kernel, αλλά αποτελεί επαρκές μέτρο για τη σύγκριση διαφορετικών αλγορίθμων στο ίδιο μηχάνημα) για τον Policy Iteration είναι ίσο με 1.57 s, ενώ για τον Value Iteration είναι ίσο με 1.67 s. Το γεγονός αυτό σε συνδυασμό με τη διαφορά 2-3 τάξεων μεγέθους στο πλήθος των απαιτούμενων επαναλήψεων, υποδεικνύει πως η χρονική πολυπλοκότητα του Policy Iteration είναι σημαντικά υψηλότερη αυτής του Value Iteration. Εξάλλου, όπως αναφέρθηκε και παραπάνω, αυτό ήταν και το σημαντικότερο κίνητρο για την ανάπτυξη του αλγορίθμου Value Iteration.</font>\n",
    "\n",
    "<!--<font color='#486393'>Για να ποσοτικοποιηθεί η διαφορά αυτή στις χρονικές πολυπλοκότητες, εάν ο χώρος των καταστάσεων $S_t$ συμβολιστεί ως $\\mathcal{S}$ και ο χώρος των προτεινόμενων βημάτων $A_t$ συμβολιστεί ως $\\mathcal{A}$, τότε η πολυπλοκότητα του Policy Iteration είναι</font>\n",
    "\n",
    "<font color='#486393'>$$\\mathcal{O}\\left({|\\mathcal{S}|^3 + |\\mathcal{A}|\\cdot|\\mathcal{S}^2|}\\right),$$</font>\n",
    "\n",
    "<font color='#486393'>ενώ η πολυπλοκότητα του Value Iteration είναι</font>\n",
    "\n",
    "<font color='#486393'>$$\\mathcal{O}\\left({|\\mathcal{A}|\\cdot|\\mathcal{S}^2|}\\right).$$</font>\n",
    "    \n",
    "<font color='#486393'>α</font>-->"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Stochastic_Processes_&_Optimization_in_Machine_Learning_(Lab_7_Markov_Decision_Processes).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
