{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHFOjD_VY0ld"
   },
   "source": [
    "<h1>Q-Learning</h1>\n",
    "\n",
    "<p>Στην συγκεκριμένη άσκηση θα μελετήσετε το στοχαστικό αλγόριθμο Q-Learning, χρησιμοποιώντας το έτοιμο πρόγραμμα που σας δίνεται.</p> <p> Στα πλαίσια του παραδείγματος θα εξετάσετε μία υλοποίηση με Q-learning σχετικά με το σύστημα αυτόματης οδήγησης ενός ταξί. Στα πλαίσια του προβλήματος αυτού θα πρέπει να ικανοποιούνται τα εξής:</p>\n",
    "<ul>\n",
    "<li>Το ταξί θα πρέπει να αφήνει τον πελάτη στη σωστή θέση</li>\n",
    "<li>Το ταξί να ακολουθεί τη συντομότερη δυνατή διαδρομή</li>\n",
    "<li>Να τηρούνται οι κανόνες κυκλοφορίας και ασφάλειας των επιβατών</li>\n",
    "</ul>\n",
    "\n",
    "<p>Στα πλαίσια του προβλήματος έχουμε τα εξής χαρακτηριστικά για την επιβράβευση, τις καταστάσεις και τις ενέργειες.</p> \n",
    "\n",
    "<h4>Επιβράβευση</h4>\n",
    "<ul>\n",
    "<li>Θα έχουμε τη μέγιστη επιβράβευση όταν το ταξί αφήνει έναν πελάτη στην επιθυμητή θέση</li>\n",
    "<li>Θα υπάρχει ποινή στην περίπτωση όπου το ταξί αφήσει τον πελάτη σε κάποιο λανθασμένο σημείο</li>\n",
    "<li>Ο agent θα παίρνει μία μικρή σχετικά ποινή στην περίπτωση όπου αργεί να φτάσει στον τελικό προορισμό</li>\n",
    "</ul>\n",
    "\n",
    "<p>Γενικά οι παραπάνω αρχές συνοψίζονται στα εξής: \"Λαμβάνουμε +20 πόντους για μια επιτυχημένη πτώση και χάνουμε 1 πόντο για κάθε χρονικό βήμα που παίρνει. Υπάρχει επίσης ποινή 10 πόντων για παράνομες ενέργειες παραλαβής και αποχώρησης.\"</p>\n",
    "\n",
    "<h4>Πλήθος Καταστάσεων</h4>\n",
    "<img src=\"https://storage.googleapis.com/lds-media/images/Reinforcement_Learning_Taxi_Env.width-1200.png\">\n",
    "<p>Στο παράδειγμά μας έχουμε ένα μικρό χώρο 5*5. Από εκεί και πέρα έχουμε 4 προορισμούς και 5 πιθανές θέσεις του πελάτη (παίρνουμε και την περίπτωση ο πελάτης να είναι ήδη μέσα στο τάξι).</p>\n",
    "<p>Με βάση τα παραπάνω έχουμε 5*5*5*4=500 πιθανές καταστάσεις.</p>\n",
    "\n",
    "<h4>Πλήθος Ενεργειών</h4>\n",
    "<p>Έχουμε έξι ενεργειες για το ταξί, οι οποίες είναι οι εξής:</p>\n",
    "<ul>\n",
    "<li>0=Νότια</li>\n",
    "<li>1=Βόρεια</li>\n",
    "<li>2=Ανατολικά</li>\n",
    "<li>3=Δυτικά</li>\n",
    "<li>4=Επιβίβαση</li>\n",
    "<li>5=Αποβίβαση</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOQpQ0DygDzt"
   },
   "source": [
    "<p>Πριν συνεχίσετε στην άσκηση να απαντήσετε στο εξής ερώτημα:</p>\n",
    "\n",
    "<b><p>1. Να περιγράψετε σύντομα τον αλγόριθμο Q-Learning. Σε ποια προβλήματα θεωρείτε ότι ταιριάζει ως τρόπος εκμάθησης η Ενισχυτική Μάθηση (Reinforcement Learning); Ποια είναι η βασική διαφορά του αλγορίθμου Q-Learning από τους αλγορίθμους Policy Iteration και Value Iteration;</p></b>\n",
    "\n",
    "<font color='#486393'>Σημειώνεται πως οι απαντήσεις στα ερωτήματα γράφονται σε αυτό το χρώμα, προκειμένου να διαφοροποιούνται από την εκφώνηση και τις οδηγίες της άσκησης.</font>\n",
    "\n",
    "<font color='#486393'>Ο αλγόριθμος Q-learning αντιστοιχεί σε έναν αλγόριθμο ενισχυτικής μάθησης ο οποίος προσπαθεί να προσεγγίσει την τιμή της βέλτιστης value function κατά Bellman, χωρίς να υπολογίζει παράλληλα κάποια βέλτιστη πολιτική, αφού δεν προϋποθέτει τη μοντελοποίηση του περιβάλλοντος του εκάστοτε προβλήματος. Προκειμένου να αποτυπωθεί μαθηματικά η αρχή λειτουργίας του αλγορίθμου, συμβολίζουμε (ακολουθώντας το φορμαλισμό των Sutton & Barto) την κατάσταση στην οποία βρίσκεται ο πράκτορας (agent) σε κάποιο βήμα, $t$, ως $S_t$, την προτεινόμενη κίνηση για το επόμενο βήμα ως $A_{t+1}$ και την πιθανή επιβράβευση λόγω της κίνησης αυτής ως $R_{t+1}$. Βάσει αυτών, ο αλγόριθμος Q-learning προσπαθεί να υπολογίσει την τιμή της βέλτιστης value function κατά Bellman μέσω της επαναληπτικής διαδικασίας</font>\n",
    "\n",
    "<font color='#486393'>$$ Q^\\text{new}\\left(S_t,A_{t+1}\\right) = Q^\\text{old}\\left(S_t,A_{t+1}\\right) + \\alpha \\cdot \\left\\{R_{t+1} + \\gamma\\left[\\text{max}_{A_{t+1}}{Q^\\text{old}\\left(S_{t+1},A_{t+1}\\right)}\\right] - Q^\\text{old}\\left(S_t,A_{t+1}\\right) \\right\\}, $$</font>\n",
    "\n",
    "<font color='#486393'>όπου $\\alpha$ είναι ο ρυθμός εκμάθησης της διαδικασίας και $\\gamma$ είναι ο ρυθμός έκπτωσης (discount factor). Όπως αναλύθηκε και στο πρώτο notebook της παρούσας εργαστηριακής αναφοράς, ο ρυθμός έκπτωσης επιτελεί την εξής λειτουργία: εάν ο πράκτορας λάβει κάποια επιβράβευση $R_{t+m}$ στο βήμα $t+m$ αντί για το βήμα $t$, τότε αυτή θα συνδέεται με την επιβράβευση, $R_{t+1}$, εάν αυτή είχε ληφθεί στο βήμα $t$, μέσω της σχέσης $R_{t+m} = \\gamma^{m-1}R_{t+1}$.</font>\n",
    "\n",
    "<font color='#486393'>Σε γενικές γραμμές, η ενισχυτική μάθηση χρησιμοποιείται σε προβλήματα τα οποία μπορούν να αποτυπωθούν ως Markov Decision Processes (MDPs), προκειμένου να αξιοποιηθούν τεχνικές του δυναμικού προγραμματισμού. Επιπλέον, παρότι και οι αλγόριθμοι της ενισχυτικής μάθησης συχνά αντιστοιχούν σε προβλήματα βελτιστοποίησης, η βελτιστοποίηση αυτή δεν νοείται με τον ίδιο τρόπο που νοείται σε προβλήματα εποπτευόμενης μάθησης, όπου το τελικό output πρέπει να αποκλίνει όσο το δυνατό λιγότερο από ένα συγκεκριμένο target value. Αντίθετα, η ενισχυτική μάθηση χρησιμοποιείται όταν κρίνεται αναγκαία η εύρεση μιας λεπτής ισορροπίας ανάμεσα στην αξιοποίηση σε υψηλό βαθμό της ήδη γνωστής πληροφορίας για το σύστημα και την εξερεύνηση νέων στρατηγικών. Οι νέες αυτές στρατηγικές σε πολλές περιπτώσεις ενδέχεται να οδηγούν και σε χαμηλότερη απόδοση σε πρώτη φάση, όμως η εξερεύνησή τους πρέπει να πραγματοποιείται, ώστε να συγκεντρώνεται νέα πληροφορία η οποία δύναται μακροπρόθεσμα να οδηγήσει σε αύξηση της απόδοσης.</font>\n",
    "\n",
    "<font color='#486393'>Τέλος, σε ό,τι αφορά την κύρια διαφορά του αλγορίθμου Q-learning από τους αλγορίθμους Policy και Value Iteration που αναλύθηκαν στο πρώτο notebook της παρούσας εργαστηριακής άσκησης, αυτή αναφέρθηκε και παραπάνω: ο αλγόριθμος Q-learning δεν πραγματοποιεί υποθέσεις για το περιβάλλον του υπό μελέτη συστήματος. Είναι, δηλαδή, ένας model-independent αλγόριθμος, γεγονός το οποίο συμβάλλει στην καθολικότητά του (δηλαδή τη χρήση του σε πληθώρα προβλημάτων) και κυρίως στη μείωση της χρονικής του πολυπλοκότητας. Η μείωση αυτή είναι συνέπεια του γεγονότος πως ο αλγόριθμος Q-learning είναι off-policy, δηλαδή δεν ασχολείται με την αρχικοποίηση ή/και βελτιστοποίηση κάποιας πολιτικής.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pc11bCDZgfSx"
   },
   "source": [
    "<p>Στη συνέχεια θα πρέπει να φορτώσετε τη βιβλιοθήκη gym καθώς και το σχετικό dataset<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NDHLb5PGWdwr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "396kTtOrW-cO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UC6-XuIhF5_"
   },
   "source": [
    "<p>Παρακάτω ορίζουμε τις συνεταγμένες του ταξί, τη θέση του πελάτη και το σημείο προορισμού</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nPSOw5CdXFx1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsAkQJhchVFy"
   },
   "source": [
    "<p>Παρακάτω είναι η μήτρα επιβράβευσης για το state που ορίσαμε στο προηγούμενο βήμα</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j7oDbznIXOJo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtROeD1ph6kk"
   },
   "source": [
    "<p> Τρέχουμε το παράδειγμά μας χωρις τη χρήση Q-Learning.</p>\n",
    "\n",
    "<b><p>2. Τα αποτελέσματα είναι ικανοποιητικά; Πως θα μας εξυπηρετούσε η χρήση του Q-Learning;</p></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vKHhcDxVXUFz"
   },
   "outputs": [],
   "source": [
    "def without_qlearning(episodes=100):\n",
    "    total_epochs, total_penalties = 0, 0\n",
    "    episodes = episodes\n",
    "    rewards = []\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        env.s = 328  # set environment to illustration's state\n",
    "\n",
    "        epochs = 0\n",
    "        penalties, reward = 0, 0\n",
    "\n",
    "        frames = [] # for animation\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, done, info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "\n",
    "            # Put each rendered frame into dict for animation\n",
    "            frames.append({\n",
    "                'frame': env.render(mode='ansi'),\n",
    "                'state': state,\n",
    "                'action': action,\n",
    "                'reward': reward\n",
    "                }\n",
    "            )\n",
    "\n",
    "            epochs += 1\n",
    "            \n",
    "        total_penalties += penalties\n",
    "        total_epochs += epochs\n",
    "        \n",
    "    if episodes==1:\n",
    "        print(\"Timesteps taken: {}\".format(epochs))\n",
    "        print(\"Penalties incurred: {}\".format(penalties))\n",
    "    else:\n",
    "        print(f\"Results after {episodes} episodes:\")\n",
    "        print(f\"Average timesteps per episode: {total_epochs / episodes:.2f}\")\n",
    "        print(f\"Average penalties per episode: {total_penalties / episodes:.2f}\")\n",
    "        rewards = np.asarray(rewards)\n",
    "        print(f\"Average reward per step: {rewards.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 727\n",
      "Penalties incurred: 236\n"
     ]
    }
   ],
   "source": [
    "without_qlearning(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZssyR6_cj9O_"
   },
   "source": [
    "<font color='#486393'>Προφανώς, τα αποτελέσματα που προκύπτουν δεν είναι ικανοποιητικά (πάνω από το 30% περίπου των κινήσεων του πράκτορα αντιστοιχούν σε σφάλμα). Ο λόγος για αυτό είναι πως για την επίλυση του προβλήματος δεν ακολουθείται κάποιος αλγόριθμος που να λαμβάνει υπ' όψιν τα σφάλματα και την ανταμοιβή που λαμβάνει ο πράκτορας κατά τη διάρκεια κάθε δρομολογίου. Αυτά, δύνανται να προσφέρουν στον πράκτορα πληροφορία την οποία μπορεί να αξιοποιήσει σε μετέπειτα δρομολόγια για να μεγιστοποιήσει την ανταμοιβή του και παράλληλα να ελαχιστοποιήσει τα σφάλματά του. Με βάση την ανάλυση που προηγήθηκε, ο αλγόριθμος Q-learning αποτελεί έναν τέτοιο αλγόριθμο, ο οποίος θα επιτρέψει στον πράκτορα να βελτιστοποιήσει τα δρομολόγιά του. Η χρήση του έναντι των αλγορίθμων που χρησιμοποιήθηκαν στο άλλο notebook της παρούσας εργαστηριακής άσκησης οφείλεται στο μειωμένο χρόνο εκτέλεσής του (όντας off-policy), χωρίς αυτός να συνεπάγεται αβεβαιότητα ως προς τη σύγκλιση, η οποία είναι εξασφαλισμένη βάσει θεωρημάτων.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aj3s09rsizVm"
   },
   "source": [
    "<p>Τώρα θα προσπαθήσουμε να λύσουμε το πρόβλημά μας με τη χρήση του Q-Learning.</p>\n",
    "\n",
    "<b><p>3. Τι γνωρίζετε για τις παραμέτρους α και γ. Τι θα συμβεί αν έχουν τιμές ίσες με 1;</p></b>\n",
    "\n",
    "<font color='#486393'>Το ερώτημα αυτό αναπτύχθηκε εν μέρει κατά την παρουσίαση του αλγορίθμου Q-learning, παρ' όλα αυτά αξίζει εδώ να επαναληφθούν κάποια βασικά σημεία και να δοθούν περισσότερες λεπτομέρειες σε άλλα.</font>\n",
    "\n",
    "<font color='#486393'>Σε ό,τι αφορά την παράμετρο $\\alpha$, αυτή αντιστοιχεί στο ρυθμό εκμάθησης της επαναληπτικής διαδικασίας. Συγκεκριμένα, η επαναληπτική διαδικασία του αλγορίθμου Q-learning σχετίζεται με την ανανέωση της value function, η οποία προκύπτει ως μια μίξη ανάμεσα στην προηγούμενη τιμή της και τον επιπλέον όρο που σχετίζεται με την ανταμοιβή του πράκτορα στο επόμενο χρονικό βήμα. Στην οριακή περίπτωση όπου $\\alpha = 0$ ο πράκτορας δρα χωρίς να μαθαίνει μέσω της αλληλεπίδρασής του με το περιβάλλον (ισοδύναμα με ό,τι παρατηρήσαμε εκτελώντας την προσομοίωση απουσία Q-learning). Στο άλλο όριο, η τιμή $\\alpha = 1$ οδηγεί σε εκμάθηση η οποία δεν κρατάει μνήμη και ως εκ τούτου μπορεί να οδηγήσει σε μια καθαρά στοχαστική συμπεριφορά, άνευ στρατηγικής.</font>\n",
    "\n",
    "<font color='#486393'>Η παράμετρος $\\gamma$ αντιστοιχεί στο ρυθμό έκπτωσης, δηλαδή το κατά πόσο η αξία της αναμενόμενης ανταμοιβής που θα λάβει ο πράκτορας μειώνεται σε βάθος χρονικών βημάτων, όπως αναλύθηκε παραπάνω. Εδώ, το όριο $\\gamma = 0$ αντιστοιχεί σε έναν «επιπόλαιο» πράκτορα, ο οποίος αποφασίζει καθαρά βάσει της δράσης που θα μεγιστοποιήσει την άμεση ανταμοιβή του. Από την άλλη, το όριο $\\gamma = 1$ αντιστοιχεί σε μια κατάσταση όπου με το πέρασμα των χρονικών βημάτων η αναμενόμενη ανταμοιβή του πράκτορα δε μειώνεται. Μάλιστα, όπως αναλύθηκε και στο άλλο notebook, στην περίπτωση όπου $\\gamma = 1$ ο αλγόριθμος δε συγκλίνει, καθώς η αναμενόμενη τιμή της value function κατά Bellman αντιστοιχεί σε μια αποκλίνουσα σειρά.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JJk3NTfcXrrA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Oy2Yg8DTXtHW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "Wall time: 29.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dxt4fmvGYBOm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.41232915,  -2.27325184,  -2.39641613,  -2.36176221,\n",
       "       -10.81546216, -10.67772066])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[328]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6W9JE9yOYGgP"
   },
   "outputs": [],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "def with_qlearning(episodes=100):\n",
    "    total_epochs, total_penalties = 0, 0\n",
    "    episodes = episodes\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        epochs, penalties, reward = 0, 0, 0\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(q_table[state])\n",
    "            state, reward, done, info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "\n",
    "            epochs += 1\n",
    "\n",
    "        total_penalties += penalties\n",
    "        total_epochs += epochs\n",
    "\n",
    "    print(f\"Results after {episodes} episodes:\")\n",
    "    print(f\"Average timesteps per episode: {total_epochs / episodes:.2f}\")\n",
    "    print(f\"Average penalties per episode: {total_penalties / episodes:.2f}\")\n",
    "    rewards = np.asarray(rewards)\n",
    "    print(f\"Average reward per step: {rewards.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5R7gW1nLj-qE"
   },
   "source": [
    "<b><p>4. Συγκρίνετε τους δύο αλγορίθμους με βάση τις παρακάτω μετρικές</p>\n",
    "<ul>\n",
    "<li>Μέσος αριθμός παραβάσεων ανά επεισόδιο</li>\n",
    "<li>Μέσος αριθμός βημάτων ανά διαδρομή</li>\n",
    "<li>Μέσος αριθμός ανταμοιβών ανά κίνηση</li>\n",
    "</ul>\n",
    "<p>Τις παραπάνω συγκρίσεις να τις κάνετε για 100 επεισόδια.</p>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#486393'>Έχοντας επεξεργαστεί ελαφρώς το δοθέντα κώδικα (γράφοντάς τον υπό τη μορφή δύο συναρτήσεων), στο ακόλουθο κελί εκτελείται η προσομοίωση 100 επεισοδίων (α) με και (β) χωρίς την εφαρμογή του αλγορίθμου Q-learning.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running simulation for 100 episodes without Q-learning...\n",
      "*********************************************************\n",
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 2470.93\n",
      "Average penalties per episode: 803.51\n",
      "Average reward per step: -3.92\n",
      "\n",
      "\n",
      "\n",
      "Running simulation for 100 episodes with Q-learning...\n",
      "******************************************************\n",
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 13.03\n",
      "Average penalties per episode: 0.00\n",
      "Average reward per step: 0.61\n"
     ]
    }
   ],
   "source": [
    "print('Running simulation for 100 episodes without Q-learning...')\n",
    "print('*'*57)\n",
    "without_qlearning(100)\n",
    "print('\\n\\n')\n",
    "print('Running simulation for 100 episodes with Q-learning...')\n",
    "print('*'*54)\n",
    "with_qlearning(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#486393'>Με βάση τα αποτελέσματα των προσομοιώσεων για 100 επεισόδια, καθίσταται εμφανές πως ο αλγόριθμος Q-learning οδηγεί σε καταπληκτικά αποτελέσματα. Στην περίπτωση των τυχαίων προσομοιώσεων, ο μέσος αριθμός παραβάσεων ανά επεισόδιο είναι περίπου ίσος με 804 παραβάσεις, σε βάθος 2471 βημάτων (κατά μέσο όρο), γεγονός που υποδεικνύει πως το 32.54% των βημάτων κάθε επεισοδίου αντιστοιχεί σε σφάλμα. Από την άλλη, ο εκπαιδευμένος πράκτορας δεν πραγματοποιεί κατά μέσο όρο κανένα σφάλμα ανά επεισόδιο, σε βάθος μόλις 13 περίπου βημάτων κατά μέσο όρο. Τα αποτελέσματα αυτά δε δείχνουν μόνο πως ο πράκτορας πραγματοποιεί σφάλματα στο 0% των βημάτων κάθε επεισοδίου, αλλά και ότι έχει βελτιστοποιήσει τη στρατηγική του ώστε να ολοκληρώνει κάθε επεισόδιο σε μόλις 13 βήματα, δηλαδή δύο τάξεις μεγέθους λιγότερα απ' ότι ο τυχαίος αλγόριθμος. Τέλος, η βελτιστοποίηση αυτή αποτυπώνεται και μέσω της μέσης ανταμοιβής ανά βήμα, η οποία ισούται με -3.92 στην περίπτωση του τυχαίου αλγορίθμου (δηλαδή συνολική ζημία) και με 0.61 στην περίπτωση του εκπαιδευμένου πράκτορα (δηλαδή συνολικό κέρδος).</font>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Stochastic_Processes_&_Optimization_in_Machine_Learning_(Lab_7_Q_Learning_2022).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
