{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Νικόλαος Μανιάτης](mailto:nikolaosmaniatis@mail.ntua.gr)\n",
    "## Α.Μ.: 03400097"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHFOjD_VY0ld"
   },
   "source": [
    "<h1>Q-Learning</h1>\n",
    "\n",
    "<p>Στην συγκεκριμένη άσκηση θα μελετήσετε το στοχαστικό αλγόριθμο Q-Learning, χρησιμοποιώντας το έτοιμο πρόγραμμα που σας δίνεται.</p> <p> Στα πλαίσια του παραδείίγματος θα εξετάσετε μίία υλοποίηση με q-learning σχετικά με το σύστημα αυτόματης οδήγησης ενός ταξί. Στα πλαίσια του προβλήματος αυτού θα πρέπει να ικανοποιούνται τα εξής:</p>\n",
    "<ul>\n",
    "<li>Το ταξί θα πρέπει να αφήνει τον πελάτη στη σωστή θέση</li>\n",
    "<li>Το ταξί να ακολουθεί τη συντομότερη δυνατή διαδρομή</li>\n",
    "<li>Να τηρούνται οι κανόόνες κυκλοφορίας και ασφάλειας των επιβατών</li>\n",
    "</ul>\n",
    "\n",
    "<p>Στα πλαίσια του προβλήματος έχουμε τα εξής χαρακτηριστικά για την επιβράβευση, τις καταστάσεις και τις ενέργειες.</p> \n",
    "\n",
    "<h4>Επιβράβευση</h4>\n",
    "<ul>\n",
    "<li>Θα έχουμε τη μέγιστη επιβράβευση όταν το ταξί αφήνει έναν πελάτη στην επιθυμητή θέση</li>\n",
    "<li>Θα υπάρχει ποινή στην περίπτωση όπου το ταξί αφήσει τον πελάτη σε κάποιο λανθασμένο σημείο</li>\n",
    "<li>Ο agent θα παίρνει μία μικρή σχετικά ποινή στην περίπτωση όπου αργεί να φτάσει στον τελικό προορισμό</li>\n",
    "</ul>\n",
    "\n",
    "<p>Γενικά οι παραπάνω αρχές συνοψίζονται στα εξής: \"Λαμβάνουμε +20 πόντους για μια επιτυχημένη πτώση και χάνουμε 1 πόντο για κάθε χρονικό βήμα που παίρνει. Υπάρχει επίσης ποινή 10 πόντων για παράνομες ενέργειες παραλαβής και αποχώρησης.\"</p>\n",
    "\n",
    "<h4>Πλήθος Καταστάσεων</h4>\n",
    "<img src=\"https://storage.googleapis.com/lds-media/images/Reinforcement_Learning_Taxi_Env.width-1200.png\">\n",
    "<p>Στο παράδειγμά μας έχουμε ένα μικρό χώρο 5*5. Από εκεί και πέρα έχουμε 4 προορισμούς και 5 πιθανές θέσεις του πελάτη (παίρνουμε και την περίπτωση ο πελάτης να είναι ήδη μέσα στο τάξι).</p>\n",
    "<p>Με βάση τα παραπάνω έχουμε 5*5*5*4=500 πιθανές καταστάσεις.</p>\n",
    "\n",
    "<h4>Πλήθος Ενεργειών</h4>\n",
    "<p>Έχουμε έξι ενεργειες για το ταξί, οι οποίες είναι οι εξής:</p>\n",
    "<ul>\n",
    "<li>0=Νότια</li>\n",
    "<li>1=Βόρεια</li>\n",
    "<li>2=Ανατολικά</li>\n",
    "<li>3=Δυτικά</li>\n",
    "<li>4=Επιβίβαση</li>\n",
    "<li>5=Αποβίβαση</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOQpQ0DygDzt"
   },
   "source": [
    "<p>Πριν συνεχίσετε στην άσκηση να απαντήσετε στο εξής ερώτημα:</p>\n",
    "\n",
    "<b><p>1. Να περιγράψετε σύντομα τον αλγόριθμο Q-Learning. Σε ποια προβλήματα θεωρείτε ότι ταιριάζει ως τρόπος εκμάθησης η Ενισχυτική Μάθηση (Reinforcement Learning); Ποια είναι η βασική διαφορά του αλγορίθμου Q-Learning από τους αλγορίθμους Policy Iteration και Value Iteration;</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ο αλγόριθμος Q-Learning λειτουργεί με το να ορίσουμε έναν πίνακα με την αξία κάθε ζεύγους state και action για το πρόβλημα που επιχειρούμε να λύσουμε, οποίος αρχικά είναι αρχικοποιημένος με μηδενικά, και γεμίζει με κάθε επανάληψη του αλγορίθμου. Υιοθετούμε μια $\\varepsilon$-greedy πολιτική, όπου με πιθανότητα $1-\\varepsilon$ ο αλγόριθμος εκτελεί την κίνηση που μεγιστοποιεί το κέρδος του (exploitation) ενώ με πιθανότητα $\\varepsilon$ εκτελεί μια τυχαία κίνηση (exploration). Έπειτα, με βάση τη γνωστή σχέση του Q-Learning ανανεώνει τις τιμές του πίνακα και συνεχίζει τις επαναλήψεις. \n",
    "Η σχέση αυτή είναι:   \n",
    "\n",
    "$$\\mathcal{Q}^{new} (s_t,a_t) \\leftarrow \\mathcal{Q}(s_t,a_t) + \\alpha(r_t + \\gamma\\cdot\\max_a \\mathcal{Q}(s_{t+1},a) - \\mathcal{Q}(s_t,a_t)) $$ \n",
    "\n",
    "H βασική διαφορά του Q-Learning από τους αλγορίθμους Policy & Value Iteration είναι ότι ο αλγόριθμος Q-Learning δεν θεωρεί γνωστές τις πιθανότητες μεταβάσεων μεταξύ των καταστάσεων ούτε των rewards. Επομένως, με την βοήθεια προσωμοιώσεων Monte Carlo προσπαθούμε να βρούμε κάποια βέλτιστη πολιτική του προβλήματος που εξετάζουμε. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pc11bCDZgfSx"
   },
   "source": [
    "<p>Στη συνέχεια θα πρέπει να φορτώσετε τη βιβλιοθήκη gym καθώς και το σχετικό dataset<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NDHLb5PGWdwr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "|\u001b[43m \u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "396kTtOrW-cO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UC6-XuIhF5_"
   },
   "source": [
    "<p>Παρακάτω ορίζουμε τις συνεταγμένες του ταξί, τη θέση του πελάτη και το σημείο προορισμού</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nPSOw5CdXFx1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsAkQJhchVFy"
   },
   "source": [
    "<p>Παρακάτω είναι η μήτρα επιβράβευσης για το state που ορίσαμε στο προηγούμενο βήμα</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "j7oDbznIXOJo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtROeD1ph6kk"
   },
   "source": [
    "<p> Τρέχουμε το παράδειγμά μας χωρις τη χρήση Q-Learning.</p>\n",
    "\n",
    "<b><p>2. Τα αποτελέσματα είναι ικανοποιητικά; Πως θα μας εξυπηρετούσε η χρήση του Q-Learning;</p></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vKHhcDxVXUFz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 1051\n",
      "Penalties incurred: 334\n"
     ]
    }
   ],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Όπως είναι προφανές τα αποτελέσματα αυτά δεν είναι ικανοποιητικά καθώς η επιλογή της επόμενης κατάστασης γίνεται ανεξάρτητα από τα rewards και τα penalties, γι'αυτό και περίπου το 1/3 των κινήσεων οδηγήθηκαν σε penalty.  \n",
    "Η χρήση του Q-Learning θα μας ήταν πολύ χρήσιμη καθώς ο αλγόριθμος μαθαίνει να κινείται διαλέγοντας κινήσεις που μεγιστοποιούν το κέρδος του και επομένως αποφεύγουν τα penalties. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aj3s09rsizVm"
   },
   "source": [
    "<p>Τώρα θα προσπαθήσουμε να λύσουμε το πρόβλημά μας με τη χρήση του Q-Learning.</p>\n",
    "\n",
    "<b><p>3. Τι γνωρίζετε για τις παραμέτρους α και γ. Τι θα συμβεί αν έχουν τιμές ίσες με 1;</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Η παράμετρος $\\alpha$ είναι το Learning Rate, αν δούμε την σχέση του Q-Learning, εκφράζει κατά τι ποσοστό η νέα πληροφορία θα κάνει overwrite την παλιά πληροφορία. Αν $\\alpha = 0$, τότε ο agent δεν μαθαίνει καθόλου, και οι τιμές του Q δεν ανανεώνονται καθόλου. Αν πάλι $\\alpha = 1$ αυτό σημαίνει ότι μαθαίνει πολύ γρήγορα, και σε ένα στοχαστικό περιβάλλον π.χ. αυτό μπορεί να μην είναι επιθυμητό. Γι'αυτό συνήθως διαλέγουμε μια σταθερή τιμή όπως στο παράδειγμά μας όπου $\\alpha = 0.1$ . \n",
    " - Η παράμετρος $\\gamma$ είναι το discount factor. Αυτό ορίζει την σημασία των μελλοντικών rewards που θα πάρει ο agent. Αν $\\gamma = 0$, τότε ο agent είναι μύωπας, δηλαδή δεν φροντίζει καθόλου για το μέλλον και παίρνει όποια απόφαση μεγιστοποιεί το άμεσο κέρδος του. Αν πάλι $\\gamma \\rightarrow 1 $, η state-value function που μας δείχνει τα αναμενόμενα κέρδη όταν ακολουθούμε μια πολιτική, και δίνεται από την σχέση $ u_{\\pi} = \\mathrm{E}\\big [ \\displaystyle \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\big| s_t =s\\big ]$, αποκλίνει, αν δεν υπάρχει τερματική κατάσταση. Επομένως, πρέπει να είναι $\\gamma < 1$ προκειμένου ο αλγόριθμος να συγκλίνει."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "JJk3NTfcXrrA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "Oy2Yg8DTXtHW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "CPU times: user 35.3 s, sys: 7.44 s, total: 42.8 s\n",
      "Wall time: 33.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "dxt4fmvGYBOm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.39346117,  -2.27325184,  -2.40801732,  -2.35679237,\n",
       "       -10.521667  , -10.68555429])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[328]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "6W9JE9yOYGgP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 13.49\n",
      "Average penalties per episode: 0.0\n",
      "Average rewards per step 0.579308\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "rewardss = []\n",
    "rewards_l = []\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        rewardss.append(reward)\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "    rewards_l.append(np.mean(rewardss))\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
    "print(\"Average rewards per step {:2f}\".format(np.mean(rewards_l)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5R7gW1nLj-qE"
   },
   "source": [
    "<b><p>4. Συγκρίνετε τους δύο αλγορίθμους με βάση τις παρακάτω μετρικές</p>\n",
    "<ul>\n",
    "<li>Μέσος αριθμός παραβάσεων ανά επεισόδιο</li>\n",
    "<li>Μέσος αριθμός βημάτων ανά διαδρομή</li>\n",
    "<li>Μέσος αριθμός ανταμοιβών ανά κίνηση</li>\n",
    "</ul>\n",
    "<p>Τις παραπάνω συγκρίσεις να τις κάνετε για 100 επεισόδια.</p>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crude_version():\n",
    "    env.s = 328  # set environment to illustration's state\n",
    "    epochs = 0\n",
    "    penalties, reward = 0, 0\n",
    "\n",
    "    frames = [] # for animation\n",
    "\n",
    "    done = False\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "            }\n",
    "        )\n",
    "\n",
    "        epochs += 1\n",
    "#     print(\"Timesteps taken: {}\".format(epochs))\n",
    "#     print(\"Penalties incurred: {}\".format(penalties))\n",
    "    return penalties,epochs,rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalties_list = []\n",
    "epochs_list = []\n",
    "rewards_list = []\n",
    "for i in range(100):\n",
    "    penalties, epochs, rewards = crude_version()\n",
    "    penalties_list.append(penalties)\n",
    "    epochs_list.append(epochs)\n",
    "    rewards_list.append(np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Number of penalties per Episode: 623.27\n",
      "Average Number of steps per Episode: 1921.30\n",
      "Average Rewards per Step: -3.85\n"
     ]
    }
   ],
   "source": [
    "print('Average Number of penalties per Episode: {:.2f}'.format(np.mean(penalties_list)))\n",
    "print('Average Number of steps per Episode: {:.2f}'.format(np.mean(epochs_list)))\n",
    "print('Average Rewards per Step: {:.2f}'.format(np.mean(rewards_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Αποτελέσματα (για 100 επεισόδια) : \n",
    " - Χωρίς χρήση Q-Learning:   \n",
    "   Μέσος αριθμός παραβάσεων ανά επεισόδιο: 623.27   \n",
    "   Μέσος αριθμός βημάτων ανά διαδρομή (επεισόδιο): 1921.30   \n",
    "   Μέσος αριθμός ανταμοιβών ανά κίνηση: -3.85   \n",
    "   \n",
    "   --------------\n",
    "   \n",
    " - Με Q-Learning:   \n",
    "   Μέσος αριθμός παραβάσεων ανά επεισόδιο: 0.0    \n",
    "   Μέσος αριθμών βημάτων ανά διαδρομή (επεισόδιο): 13.49    \n",
    "   Μέσος αριθμός ανταμοιβών ανά κίνηση: 0.58    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Είναι εμφανές ότι ο αλγόριθμος Q-Learning φέρνει εντυπωσιακά αποτελέσματα σε σχέση με τον απλό αλγόριθμο που είναι random, καθώς ο Q-Learning έχει την ικανότητα να μαθαίνει από την εκπαίδευσή του, και στο inference έχει μάθει για κάθε κατάσταση ποιες είναι οι ιδανικές ενέργειες, και έτσι καταφέρνει και μεγιστοποιεί το κέρδος του, εκμηδενίζοντας τον αριθμό των ποινών."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Stochastic_Processes_&_Optimization_in_Machine_Learning_(Lab_6_Q_Learning_2021).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
