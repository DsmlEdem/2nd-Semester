{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Νικόλαος Μανιάτης](mailto:nikolaosmaniatis@mail.ntua.gr)\n",
    "## Α.Μ.: 03400097\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XLZdEbAy2jfr"
   },
   "source": [
    "<h1><b>Markov Decision Processes</h1></b>\n",
    "<p align=\"justify\">Στη συγκεκριμένη άσκηση θα μελετήσετε τους αλγορίθμους <i>Policy Iteration</i> και <i>Value Iteration</i>, καθώς και θα εξοικειωθείτε με βασικές έννοιες των <i>Markov Decision Processes</i>. Οι αλγόριθμοι <i>Policy Iteration</i> και <i>Value Iteration</i> είναι από τους βασικούς αλγορίθμους δυναμικού προγραμματισμού που χρησιμοποιούνται για την επίλυση της εξίσωσης <i>Bellman</i> σε <i>Markov Decision Processes</i>.</p> \n",
    "<p align=\"justify\">Το πρόβλημα που θα μελετήσετε είναι αυτό της παγωμένης λίμνης (Frozen Lake) με μέγεθος πλέγματος 8 x 8.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6VsUV229__zO"
   },
   "source": [
    "<h2><b>Εξοικείωση με τη βιβλιοθήκη <i>Gym</i></b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OM8ivgOJAg_H"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_puV3ugeAnbU"
   },
   "source": [
    "Με την παρακάτω εντολή, ορίζετε το πρόβλημα που θα μελετηθεί:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ep-MvIUCAxT8"
   },
   "outputs": [],
   "source": [
    "env_name = 'FrozenLake8x8-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBKBXJDUBRUh"
   },
   "source": [
    "Με τις παρακάτω εντολές, θα επαναφέρετε τον Agent στην αρχική του θέση και θα οπτικοποιήσετε το πλέγμα και τη θέση του Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6lqbG9zBgdi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FX2res4JBlYb"
   },
   "source": [
    "Με τις παρακάτω εντολές, ορίζετε την επόμενη ενέργεια με τυχαίο τρόπο και ο Agent κάνει ένα βήμα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gq7q944YBx0Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "next_action = env.action_space.sample()\n",
    "env.step(next_action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mV4A7lsLB54y"
   },
   "source": [
    "Να εκτελέσετε αρκετές φορές τις τελευταίες εντολές και να παρατηρήσετε κάθε φορά την ενέργεια που ζητείται από τον agent να εκτελέσει και την ενέργεια που αυτός πραγματοποιεί. Πραγματοποιεί πάντοτε ο agent την κίνηση που του ζητείται; Είναι ντετερμινιστικές ή στοχαστικές οι κινήσεις του agent;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------\n",
    "Παρατηρούμε ότι ο agent παραπάνω, δεν κάνει πάντα την κίνηση που του ζητείται. Αυτό συμβαίνει διότι το Frozen Lake Environment προσωμοιώνει μια ολισθηρή επιφάνεια. Αυτό σημαίνει ότι οι μεταβάσεις από την μια κατάσταση στην άλλη είναι στοχαστικές, δηλαδή η μετάβαση του agent σε κάθε πιθανή διεύθυνση δίνεται από κάποια πιθανότητα. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAL4we3gDV_J"
   },
   "source": [
    "<h2><b>Ερωτήσεις</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQKm4VAUChi1"
   },
   "source": [
    "<ul>\n",
    "<li>Μελετώντας <a href=\"https://gym.openai.com/envs/FrozenLake-v0/\">αυτό</a> και βασισμένοι στα συμπεράσματα του προηγούμενου ερωτήματος να περιγράψετε σύντομα το πρόβλημα της παγωμένης λίμνης ως πρόβλημα βελτιστοποίησης. Ποιος είναι ο στόχος του agent;</li>\n",
    "<li>Να διατυπώσετε την ιδιότητα <i>Markov</i>. Πώς απλοποιεί η ιδιότητα αυτή τη μελέτη του συγκεκριμένου προβλήματος;</li>\n",
    "<li>Να περιγράψετε σύντομα τους αλγορίθμους <i>Policy Iteration</i> και <i>Value Iteration</i>, δίνοντας έμφαση στο διαφορετικό τρόπο με τον οποίο προσεγγίζουν την επίλυση του προβλήματος. Είναι εγγυημένο ότι οι δύο αλγόριθμοι θα συγκλίνουν στη βέλτιστη πολιτική; Αν ναι, οδηγούν σε ίδια ή διαφορετική βέλτιστη πολιτική;</li>\n",
    "<li>Να εκτελέσετε τα προγράμματα που σας δίνονται, τα οποία επιλύουν το\n",
    "πρόβλημα της παγωμένης λίμνης, χρησιμοποιώντας τους αλγορίθμους <i>Policy\n",
    "Iteration</i> και <i>Value Iteration</i> αντίστοιχα. Ποια μέθοδος συγκλίνει στη βέλτιστη λύση σε λιγότερα βήματα; Τι συμπέρασμα βγάζετε; Να ελέγξετε το χρόνο εκτέλεσης του κάθε προγράμματος, χρησιμοποιώντας την εντολή <i>time</i>. Τι συμπέρασμα βγάζετε ως προς την πολυπλοκότητα του κάθε αλγορίθμου;</li>\n",
    "</ul>\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Απαντήσεις"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - > *Μελετώντας αυτό και βασισμένοι στα συμπεράσματα του προηγούμενου ερωτήματος να περιγράψετε σύντομα το πρόβλημα της παγωμένης λίμνης ως πρόβλημα βελτιστοποίησης. Ποιος είναι ο στόχος του agent;*\n",
    " \n",
    "Στο πρόβλημα της παγωμένης λίμνης ο agent ελέγχει την κίνηση ενός χαρακτήρα στο $8\\times8$ πλέγμα μιας παγωμένης λίμνης. Ο χαρακτήρας μπορεί να περπατήσει σε κάποια σημεία του πλέγματος, στα S,F όπου S: starting point (safe) και F: Frozen surface (safe), ενώ αν περάσει σε σημείο H, τότε μένει εκεί καθώς H: Hole οπότε πέφτει στην τρύπα. **Στόχος** του agent είναι να φτάσει στο G: Goal.   \n",
    "Όσον αφορά τα rewards, ο agent παίρνει +1 κάθε φορά που φτάνει στο G και 0 σε κάθε άλλο σενάριο. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- > *Να διατυπώσετε την ιδιότητα Markov. Πώς απλοποιεί η ιδιότητα αυτή τη μελέτη του συγκεκριμένου προβλήματος;*\n",
    "\n",
    "Μια στοχαστική διαδικασία ικανοποιεί την ιδιότητα Markov όταν οι μελλοντικές καταστάσεις της διαδικασίας, εξαρτώνται μόνο από την παρούσα κατάσταση. Δηλαδή, το μέλλον δεν εξαρτάται από το παρελθόν της διαδικασίας παρά μόνο από το παρόν. Αυτό απλοποιεί το πρόβλημα μας καθώς αν ισχύει η ιδιότητα Markov τότε εφόσον οι state and action spaces είναι πεπερασμένοι, το Reinforcement Learning task που έχουμε γίνεται finite Markov Decision Process. Έτσι, δεδομένης οποιασδήποτε state $s$ και action $\\alpha$, τότε για το επόμενο βήμα, η πιθανότητα οποιουδήποτε πιθανού συνδιασμού state, action $s',\\alpha'$ καθώς και reward $r$ δίνεται από τη σχέση:\n",
    "$$p\\left(s^{\\prime}, r \\mid s, a\\right)=\\operatorname{Pr}\\left\\{S_{t+1}=s^{\\prime}, R_{t+1}=r \\mid S_{t}=s, A_{t}=a\\right\\} $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - > *Να περιγράψετε σύντομα τους αλγορίθμους Policy Iteration και Value Iteration, δίνοντας έμφαση στο διαφορετικό τρόπο με τον οποίο προσεγγίζουν την επίλυση του προβλήματος.*\n",
    " \n",
    "Οι αλγόριθμοι Policy & Value Iteration είναι δυο διαφορετικοί αλγόριθμοι υπολογισμού βέλτιστης πολιτικής για ένα Reinforcement Learning Task.  \n",
    "\n",
    " - Ο αλγόριθμος **Policy Iteration** έχει δύο βασικά βήματα, το **Policy Evaluation** και το **Policy Improvement**.\n",
    "Το βήμα του Policy Evaluation περιλαμβάνει τον υπολογισμό της state-value function $u_{\\pi}$ (ουσιαστικά δηλώνει το αναμενόμενο κέρδος που θα επιφέρει μια πολιτική που ακολουθούμε) για μια αυθαίρετη πολιτική $\\pi$, αυτό γίνεται αναδρομικά μέσω της σχέσης:\n",
    "$$u_{\\pi}(s) = \\displaystyle\\sum_{a} \\pi(a \\mid s) \\displaystyle\\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right]$$\n",
    "επομένως αθροίζει πάνω σε κάθε κατάσταση, action και reward της πολιτικής, και η επανάληψη τερματίζεται όταν η Value function αλλάζει κατά έναν πολύ μικρό αριθμό.\n",
    "Στη συνέχεια, έχοντας υπολογίσει την $u_{\\pi}$, θέλουμε να μάθουμε για μια συγκεκριμένη κατάσταση αν είναι καλή ιδέα να αλλάξουμε την πολιτική επιλέγοντας μια $\\alpha \\neq \\pi(s)$.   \n",
    "Έτσι, αν έχουμε δύο πολιτικές $\\pi$ και $\\pi'$, μπορούμε να κάνουμε τον έλεγχο $q_{\\pi}\\left(s, \\pi^{\\prime}(s)\\right) \\geq v_{\\pi}(s)$ όπου $q_{\\pi}(s,\\alpha) =\\displaystyle\\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right]$. Αν ισχύει αυτή η ανισότητα τότε η πολιτική $\\pi'$ θα πρέπει να είναι καλύτερη ή εξίσου καλή με την $\\pi$, δηλαδή θα επιφέρει μεγαλύτερα (ή ίσα) κέρδη από rewards, για όλες τις καταστάσεις. Τέλος, ο αλγόριθμος Policy Iteration έχει διαδοχικές επαναλήψεις από Policy Evaluation και Policy Improvement, δηλαδή υπολογίζεται η state-value function μιας πολιτικής, στη συνέχεια εξετάζεται το ενδεχόμενο βελτίωσης, κ.ο.κ.   \n",
    "Σχηματικά: $$\\pi_{0} \\stackrel{\\mathrm{E}}{\\longrightarrow} v_{\\pi_{0}} \\stackrel{\\mathrm{I}}{\\longrightarrow} \\pi_{1} \\stackrel{\\mathrm{E}}{\\longrightarrow} v_{\\pi_{1}} \\stackrel{\\mathrm{I}}{\\longrightarrow} \\pi_{2} \\stackrel{\\mathrm{E}}{\\longrightarrow} \\cdots \\stackrel{\\mathrm{I}}{\\longrightarrow} \\pi_{*} \\stackrel{\\mathrm{E}}{\\longrightarrow} v_{*} $$ \n",
    "όπου το βέλος $\\stackrel{\\mathrm{E}}{\\longrightarrow}$ υποδηλώνει Policy Evaluation και το βέλος $\\stackrel{\\mathrm{I}}{\\longrightarrow}$ Policy Improvement. Κάθε επανάληψη Policy Evaluation ξεκινάει με την value function της προηγούμενης πολιτικής. \n",
    "\n",
    "- Ο αλγόριθμος **Value-Iteration**, αποσκοπεί στη βελτίωση της Policy Iteration εξαιτίας του γεγονότος ότι το Policy Evaluation κομμάτι της είναι ακριβό υπολογιστικά καθώς έχει πολλά περάσματα στο χώρο των καταστάσεων. Περιλαμβάνει ένα πέρασμα Policy Evaluation και ένα πέρασμα Policy Improvement. Συγκεκριμένα, η διαδικασία του policy evaluation σταματάται μετά από ένα πέρασμα του χώρου καταστάσεων, και αποσκοπεί στην εύρεση της optimal value function, δηλαδή:\n",
    "$$u_{k+1}(s) =\\max _{a} \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{k}\\left(s^{\\prime}\\right)\\right]$$\n",
    "Αυτή η αναδρομική διαδικασία σταματάει όταν η $u$ αλλάζει κατά ένα πολύ μικρό αριθμό, που ορίζεται από τον προγραμματιστή σαν ένα κατώφλι. Τέλος, ο αλγόριθμος στην έξοδο δίνει μια ντετερμινιστική πολιτική (policy extraction). Δεν υπάρχει επανάληψη αυτών των δυο διαδικασιών καθώς αν η value function είναι βέλτιστη, τότε και η πολιτική που θα εξαχθεί θα είναι βέλτιστη.  \n",
    "\n",
    "Οι δύο αλγόριθμοι είναι όμοιοι, όμως η **ειδοποιός διαφορά** είναι ότι ο Policy Iteration ξεκινάει με μια πολιτική, υπολογίζει την Value function για αυτή την πολιτική, και στην πορεία βρίσκει μια άλλη σίγουρα καλύτερη (ή το ίδιο, αν είναι βέλτιστη) πολιτική. Αντιθέτως, ο Value Iteration ξεκινάει από μια Value function, και βρίσκει μια καλύτερη Value function μέχρι να βρει την βέλτιστη, έτσι και η πολιτική που θα προκύπτει από αυτή την value function θα είναι βέλτιστη. Επομένως ο Policy Iteration εκτελεί επαναλήψεις πάνω στις πολιτικές ενώ o Value Iteration πάνω στις Value functions, όπως υποδηλώνουν και τα ονόματά των αλγορίθμων, αν και δεν είναι εμφανές αυτό αν δεν γνωρίζει κάποιος πως δουλεύει ο κάθε αλγόριθμος.\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " > *Είναι εγγυημένο ότι οι δύο αλγόριθμοι θα συγκλίνουν στη βέλτιστη πολιτική; Αν ναι, οδηγούν σε ίδια ή διαφορετική βέλτιστη πολιτική;* \n",
    " \n",
    "Όσον αφορά την **σύγκλιση**: \n",
    "Ο αλγόριθμος Policy Iteration είναι εγγυημένο ότι θα συγκλίνει, καθώς σε κάθε βήμα η πολιτική βελτιώνεται, αυτό σημαίνει ότι δεδομένου ότι μπορούμε να συναντήσουμε κάθε πολιτική το πολύ μια φορά, τότε όταν θα έχουμε κάνει επαναλήψεις τόσες όσες διαφορετικές πολιτικές υπάρχουν, δηλαδή επαναλήψεις ίσες με $\\#states \\times \\#actions$. \n",
    "Ομοίως, τυπικά και ο Value Iteration είναι εγγυημένο ότι συγκλίνει ασυμπτωτικά, πρακτικά μετά από μερικές επαναλήψεις όπου αλλάζει η value function κατά πολύ λίγο.   \n",
    "Όσον αφορά την **βέλτιστη πολιτική**:   \n",
    "Για τον αλγόριθμο Policy Iteration, στη σύγκλιση θα ισχύει ότι $\\pi_{k+1} (s) = \\pi_k (s) \\ \\forall s \\in \\mathcal{S}$. Αυτό σημαίνει ότι ικανοποιεί την εξίσωση Bellman και ότι η πολιτική είναι βέλτιστη, δηλαδή: \n",
    "$u_{\\pi_k} = u_*$. Για τον αλγόριθμο Value Iteration, η σύγκλισή του συνεπάγεται ότι η Value Function συγκλίνει στη βέλτιστη Value function, άρα και σε βέλτιστη πολιτική, γιατί πάλι ικανοποιείται η εξίσωση Bellman.   \n",
    "Όσον αφορά αν οδηγούν σε **ίδια ή διαφορετική βέλτιστη πολιτική**:   \n",
    "Και στις δύο περιπτώσεις, οι πολιτικές υπολογίζονται από value functions με τον ίδιο τρόπο. Επομένως αν οι πολιτικές είναι παρόμοιες μπορεί να νομίζουμε ότι έχουν προέλθει από παρόμοιες value functions. Αλλά στη γενική περίπτωση κάτι τέτοιο μπορεί να μην ισχύει, διότι μια βέλτιστη πολιτική μπορεί να προέρχεται από μη-βέλτιστη value function. Αυτή ήταν και η αρχική έμπνευση του αλγορίθμου Value Iteration. Επομένως, η απάντηση είναι ότι πιθανόν να οδηγούν σε ίδια πολιτική αλλά στη γενική περίπτωση κάτι τέτοιο δεν ισχύει, μπορεί όμως, οι δύο διαφορετικές πολιτικές να είναι εξίσου βέλτιστες."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - > *Να εκτελέσετε τα προγράμματα που σας δίνονται, τα οποία επιλύουν το πρόβλημα της παγωμένης λίμνης, χρησιμοποιώντας τους αλγορίθμους Policy Iteration και Value Iteration αντίστοιχα. Ποια μέθοδος συγκλίνει στη βέλτιστη λύση σε λιγότερα βήματα; Τι συμπέρασμα βγάζετε; Να ελέγξετε το χρόνο εκτέλεσης του κάθε προγράμματος, χρησιμοποιώντας την εντολή time. Τι συμπέρασμα βγάζετε ως προς την πολυπλοκότητα του κάθε αλγορίθμου;*\n",
    " \n",
    "Παρατηρούμε ότι η μέθοδος Policy Iteration συγκλίνει σε μόλις 12 βήματα (κάποιες φορές και σε λιγότερα), ενώ ο αλγόριθμος Value Iteration σε 2357 βήματα, επομένως η policy iteration συγκλίνει σε λιγότερα βήματα. Αυτό είναι αναμενόμενο καθώς ο policy iteration χρειάζεται λίγες επαναλήψεις να συγκλίνει, δεδομένου ότι η πολιτική σε κάθε επανάληψη γίνεται καλύτερη, και υπάρχουν το πολύ 256 διαφορετικές πολιτικές. Ενώ ο value iteration θεωρητικά χρειάζεται άπειρες επαναλήψεις για να συγκλίνει, που πρακτικά εμείς το τερματίζουμε όταν το $eps<10^{-20}$. Στο πρώτο Run που κάναμε, με τη χρήση της time, η policy iteration πήρε 1.63 s να συγκλίνει, ενώ η value iteration 1.52 s, όμως αυτοί οι χρόνοι αλλάζουν κάθε φορά που το ξανατρέχουμε και κάποιες φορές η Policy Iteration είναι ταχύτερη (κάποιες φορές συγκλίνει και σε λιγότερες επαναλήψεις) επομένως καταλαβαίνουμε ότι λόγω του ότι δεν υπάρχει κάποιo random seed οι χρόνοι είναι μεταβαλλόμενοι, όμως κειμένονται στην ίδια τάξη μεγέθους. Άρα, αν υποθέσουμε ότι συγκλίνουν σε παρόμοιο χρόνο, και ο Value Iteration παίρνει μια τάξη μεγέθους παραπάνω επαναλήψεις για να συγκλίνει, το συμπέρασμα είναι ότι ο Value Iteration είναι χαμηλότερης πολυπλοκότητας. Πράγματι, αν ο χώρος καταστάσεων (space state) είναι διάστασης $|\\mathcal{S}|$ και ο action space είναι διάστασης $|\\mathcal{A}|$, τότε οι πολυπλοκότητες είναι ([πηγή](https://cs.uwaterloo.ca/~ppoupart/teaching/cs886-spring13/slides/cs886-module7-policy-iteration.pdf)):   \n",
    "<div align=\"center\"> <b>Policy Iteration: $\\mathcal{O}(\\mathcal{S}|^3 + |\\mathcal{S}|^2|\\mathcal{A}|)$</div> \n",
    "<div align=\"center\"> <b>Value Iteration: $\\mathcal{O}(|\\mathcal{S}|^2|\\mathcal{A}|)$</div> \n",
    "Επομένως το γεγονός ότι ο policy iteration απαιτεί λιγότερες επαναλήψεις από ότι ο value iteration προκαλεί το να έχουμε παρόμοιο runtime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S6mci5P4HJ_1"
   },
   "source": [
    "<h2><b>Policy Iteration</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_43MjfJ9G8v7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy-Iteration converged at step 7.\n",
      "Average scores =  1.0\n",
      "CPU times: user 1.07 s, sys: 34.7 ms, total: 1.11 s\n",
      "Wall time: 1.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Solving FrozenLake8x8 environment using Policy iteration.\n",
    "Author : Moustafa Alzantot (malzantot@ucla.edu)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "\n",
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Runs an episode and return the total reward \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0, n = 100):\n",
    "    scores = [run_episode(env, policy, gamma, False) for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            q_sa[a] = sum([p * (r + gamma * v[s_]) for p, s_, r, _ in  env.P[s][a]])\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "def compute_policy_v(env, policy, gamma=1.0):\n",
    "    \"\"\" Iteratively evaluate the value-function under policy.\n",
    "    Alternatively, we could formulate a set of linear equations in iterms of v[s] \n",
    "    and solve them to find the value function.\n",
    "    \"\"\"\n",
    "    v = np.zeros(env.nS)\n",
    "    eps = 1e-10\n",
    "    while True:\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            policy_a = policy[s]\n",
    "            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])\n",
    "        if (np.sum((np.fabs(prev_v - v))) <= eps):\n",
    "            # value converged\n",
    "            break\n",
    "    return v\n",
    "\n",
    "def policy_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Policy-Iteration algorithm \"\"\"\n",
    "    policy = np.random.choice(env.nA, size=(env.nS))  # initialize a random policy\n",
    "    max_iterations = 200000\n",
    "    gamma = 1.0\n",
    "    for i in range(max_iterations):\n",
    "        old_policy_v = compute_policy_v(env, policy, gamma)\n",
    "        new_policy = extract_policy(old_policy_v, gamma)\n",
    "        if (np.all(policy == new_policy)):\n",
    "            print ('Policy-Iteration converged at step %d.' %(i+1))\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_name  = 'FrozenLake8x8-v0'\n",
    "    env = gym.make(env_name)\n",
    "    env = env.unwrapped\n",
    "    optimal_policy = policy_iteration(env, gamma = 1.0)\n",
    "    scores = evaluate_policy(env, optimal_policy, gamma = 1.0)\n",
    "    print('Average scores = ', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcikBq6BHRQM"
   },
   "source": [
    "<h2><b>Value Iteration</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gHvcnTDcHGmH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  1.0\n",
      "CPU times: user 1.49 s, sys: 54.6 ms, total: 1.55 s\n",
      "Wall time: 1.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Solving FrozenLake8x8 environment using Value-Itertion.\n",
    "Author : Moustafa Alzantot (malzantot@ucla.edu)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "\n",
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
    "    total reward.\n",
    "    args:\n",
    "    env: gym environment.\n",
    "    policy: the policy to be used.\n",
    "    gamma: discount factor.\n",
    "    render: boolean to turn rendering on/off.\n",
    "    returns:\n",
    "    total reward: real value of the total reward recieved by agent under policy.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0,  n = 100):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "            run_episode(env, policy, gamma = gamma, render = False)\n",
    "            for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for next_sr in env.P[s][a]:\n",
    "                # next_sr is a tuple of (probability, next state, reward, done)\n",
    "                p, s_, r, _ = next_sr\n",
    "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def value_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Value-iteration algorithm \"\"\"\n",
    "    v = np.zeros(env.nS)  # initialize value-function\n",
    "    max_iterations = 100000\n",
    "    eps = 1e-20\n",
    "    for i in range(max_iterations):\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            q_sa = [sum([p*(r + prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.nA)] \n",
    "            v[s] = max(q_sa)\n",
    "        if (np.sum(np.fabs(prev_v - v)) <= eps):\n",
    "            print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
    "            break\n",
    "    return v\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_name  = 'FrozenLake8x8-v0'\n",
    "    gamma = 1.0\n",
    "    env = gym.make(env_name)\n",
    "    env = env.unwrapped\n",
    "    optimal_v = value_iteration(env, gamma);\n",
    "    policy = extract_policy(optimal_v, gamma)\n",
    "    policy_score = evaluate_policy(env, policy, gamma, n=1000)\n",
    "    print('Policy average score = ', policy_score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Stochastic_Processes_&_Optimization_in_Machine_Learning_(Lab_6_Markov_Decision_Processes).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
