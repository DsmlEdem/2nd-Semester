{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rslab-ntua/MSc_GBDA/blob/master/2022/Lab5.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data, unzip\n",
    "!gdown --fuzzy https://drive.google.com/file/d/1pgTcsgGwogtc4EPy1I2mOtMiB-Hl13Iy/view?usp=sharing\n",
    "!tar -xf dataset.tar.gz --directory ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and feeding pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, default_collate\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.transforms.functional import convert_image_dtype\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "DATA_ROOT = \"./dataset\"\n",
    "\n",
    "# Define a class to handle data loading from disk\n",
    "class ODDataset(Dataset):\n",
    "    def __init__(self, data_root, mode=\"train\"):\n",
    "        '''\n",
    "        data_root: path to the root directory of the dataset\n",
    "        mode: [\"train\"(default)/\"val\"/\"test\"] available modes/splits of the dataset\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        assert mode in [\"train\", \"val\", \"test\"]\n",
    "\n",
    "        self.mode = mode\n",
    "        self.root = data_root\n",
    "        \n",
    "        # Build dataset\n",
    "        self._build_db()\n",
    "        \n",
    "        # Define dataset nomenclanture\n",
    "        self.categories = {\n",
    "            \"battery\": 0,\n",
    "            \"dice\": 1,\n",
    "            \"toycar\": 2,\n",
    "            \"candle\": 3,\n",
    "            \"highlighter\": 4,\n",
    "            \"spoon\": 5\n",
    "        }\n",
    "        \n",
    "    @property\n",
    "    def reverse_categories(self) -> dict:\n",
    "        '''\n",
    "        Returns a category id to category name mapping as a dict\n",
    "        '''\n",
    "        return {v:k for k, v in self.categories.items()}\n",
    "        \n",
    "        \n",
    "    def _build_db(self) -> None:\n",
    "        '''\n",
    "        Collect a database of all available samples\n",
    "        '''\n",
    "        self.db = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{self.mode}_test.txt\"),\n",
    "            header=None\n",
    "        ).values.flatten().tolist()\n",
    "        \n",
    "    def _parse_sample(self, sample: str) -> tuple:\n",
    "        '''\n",
    "        Read image and object detection label data \n",
    "        '''\n",
    "        \n",
    "        im_file = os.path.join(self.root, self.mode, \"images\", sample)\n",
    "        assert os.path.exists(im_file)\n",
    "        \n",
    "        label_file = os.path.join(self.root, self.mode, \"labels\", sample.replace(\".jpg\", \".txt\"))\n",
    "        assert os.path.exists(label_file)\n",
    "        \n",
    "        # read image from disk and convert to [0,1] float32\n",
    "        im = convert_image_dtype(read_image(im_file, mode=ImageReadMode.RGB), dtype=torch.float32)\n",
    "        \n",
    "        # read label in KITTI format and parse labels and bboxes as list of dict {\"labels\", \"boxes\"}\n",
    "        # boxes in xyxy format (left, top, right, bottom)\n",
    "        labels_kitti = pd.read_csv(label_file, sep=\" \", header=None).values\n",
    "        labels = default_collate([{\n",
    "            \"boxes\": torch.from_numpy(row_record[4:8].astype(np.int64)),\n",
    "            \"labels\": self.categories[row_record[0]]\n",
    "        } for row_record in labels_kitti ])\n",
    "        \n",
    "        return im, labels\n",
    "    \n",
    "    def __getitem__(self, index) -> tuple:\n",
    "        '''\n",
    "        Retrieve a specific sample from the dataset\n",
    "        '''\n",
    "        sample = self.db[index]\n",
    "        return self._parse_sample(sample)\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns the total number of samples in the dataset\n",
    "        '''\n",
    "        return len(self.db)\n",
    "\n",
    "# Initialize a dataset instance for testing purposes\n",
    "dset = ODDataset(DATA_ROOT, mode=\"train\")\n",
    "\n",
    "# Visualize sample #0\n",
    "sample = dset[0]\n",
    "\n",
    "# Draw bounding boxes on image\n",
    "drawn_image = draw_bounding_boxes(\n",
    "    image=convert_image_dtype(sample[0], torch.uint8),\n",
    "    boxes=sample[1][\"boxes\"],\n",
    "    labels=[dset.reverse_categories[int(cat_id)] for cat_id in sample[1][\"labels\"]],\n",
    ")\n",
    "plt.imshow(drawn_image.permute(1,2,0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "\n",
    "train_dset = ODDataset(DATA_ROOT, mode=\"train\")\n",
    "val_dset = ODDataset(DATA_ROOT,mode=\"val\")\n",
    "test_dset = ODDataset(DATA_ROOT, mode=\"test\")\n",
    "\n",
    "# Define an appropriate DataLoader\n",
    "\n",
    "# Specify a custom collate function to perform manual batching\n",
    "def custom_collate(samples: List[Tuple[torch.Tensor, dict]]) -> Tuple[List[torch.Tensor], List[dict]]:\n",
    "    '''\n",
    "    samples: a list of tuples. Each tuple has a RGB image as a torch.Tensor and a dict with the following:\n",
    "                    -- \"boxes\" : Nx4 boxes found in each image\n",
    "                    -- \"labels\": N, categories of each object found\n",
    "    \n",
    "    Returns (tuple):\n",
    "        --  List of images (variable sizes allowed) of type torch.Tensor\n",
    "        --  List of dict with the following data:\n",
    "                -- \"boxes\" : Nx4 boxes found in each image\n",
    "                -- \"labels\": N, categories of each object found\n",
    "    '''\n",
    "    images = [s[0] for s in samples]\n",
    "    labels = [s[1] for s in samples]\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# Define dataloaders for train/val/test splits\n",
    "train_dloader = DataLoader(train_dset, batch_size=8, shuffle=True, collate_fn=custom_collate, num_workers=8, prefetch_factor=2)\n",
    "val_dloader = DataLoader(val_dset, batch_size=8, shuffle=False, collate_fn=custom_collate, num_workers=8, prefetch_factor=2)\n",
    "test_dloader = DataLoader(test_dset, batch_size=8, shuffle=False, collate_fn=custom_collate, num_workers=8, prefetch_factor=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define an OD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "\n",
    "# Define a Faster RCNN model\n",
    "class FasterRCNNModel(pl.LightningModule):\n",
    "    def __init__(self, num_classes):\n",
    "        '''\n",
    "        num_classes: Number of target classes in the dataset\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # Use a pretrained backbone with \"new\" ROI head\n",
    "        self.model = fasterrcnn_resnet50_fpn(\n",
    "            pretrained=False, \n",
    "            progress=False, \n",
    "            num_classes=num_classes, \n",
    "            pretrained_backbone=True, \n",
    "            trainable_backbone_layers=True)\n",
    "        self.training_phase: Optional[int] = None\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        \n",
    "    def forward(self, x: List[torch.Tensor]) -> List[dict]:\n",
    "        '''\n",
    "        x: List of images (any size)\n",
    "        \n",
    "        Returns:\n",
    "        - List of dicts with keys (one dict per input image):\n",
    "            -- \"boxes\" : Nx4 boxes found in each image\n",
    "            -- \"labels\" : N, categories of each object found\n",
    "            -- \"scores\" : N, prediction scores for each object found\n",
    "        '''\n",
    "        assert not self.training, \"Use forward only for inference!\"\n",
    "        \n",
    "        return self.model(x)\n",
    "    \n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        '''\n",
    "        Training logic\n",
    "        '''\n",
    "        images, labels = batch\n",
    "        \n",
    "        # When in train mode FRCNN returns a dict of losses\n",
    "        #    -- loss_objectness (RPN)\n",
    "        #    -- loss_rpn_box_reg (RPN)\n",
    "        #    -- loss_classifier (ROI Heads)\n",
    "        #    -- loss_box_reg (ROI Heads)\n",
    "        losses = self.model(images, labels) \n",
    "    \n",
    "        # Reduce by sum and return the appropriate composite loss function according to the current training phase\n",
    "        assert self.training_phase is not None\n",
    "        if self.training_phase % 2 == 0:\n",
    "            # Phase 0 or 2\n",
    "            self.log(\"loss/objectness\", losses[\"loss_objectness\"], batch_size=len(images), on_epoch=True, on_step=False)\n",
    "            self.log(\"loss/rpn_box_reg\", losses[\"loss_rpn_box_reg\"], batch_size=len(images), on_epoch=True, on_step=False)\n",
    "            return sum([losses[\"loss_objectness\"], losses[\"loss_rpn_box_reg\"]])\n",
    "        else:\n",
    "            # Phase 1 or 3 \n",
    "            self.log(\"loss/classifier\", losses[\"loss_classifier\"], batch_size=len(images), on_epoch=True, on_step=False)\n",
    "            self.log(\"loss/box_reg\", losses[\"loss_box_reg\"], batch_size=len(images), on_epoch=True, on_step=False)\n",
    "            return sum([losses[\"loss_classifier\"], losses[\"loss_box_reg\"]])\n",
    "        \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        '''\n",
    "        Define optimizer and trainable parameters according to current training phase\n",
    "        '''\n",
    "        print(f\"Configuring optimizers for training phase : {self.training_phase} / 3\")\n",
    "        assert self.training_phase is not None, \"Set training phase first!\"\n",
    "        if self.training_phase == 0:\n",
    "            # Train only RPN + backbone\n",
    "            return torch.optim.Adam(\n",
    "                params=[\n",
    "                    {\"params\": self.model.backbone.parameters()},\n",
    "                    {\"params\":self.model.rpn.parameters()}\n",
    "                ],\n",
    "                lr = 1e-4\n",
    "            )\n",
    "        elif self.training_phase == 1:\n",
    "            # Train only RoiHeads + backbone\n",
    "            return torch.optim.Adam(\n",
    "                params=[\n",
    "                    {\"params\": self.model.backbone.parameters()},\n",
    "                    {\"params\":self.model.roi_heads.parameters()}\n",
    "                ],\n",
    "                lr = 1e-4\n",
    "            )\n",
    "        elif self.training_phase == 2:\n",
    "            # Train only RPN \n",
    "            return torch.optim.Adam(\n",
    "                params=self.model.rpn.parameters(), #type: ignore\n",
    "                lr = 1e-4\n",
    "            )\n",
    "        elif self.training_phase == 3:\n",
    "            # Train only RoiHeads \n",
    "            return torch.optim.Adam(\n",
    "                params=self.model.roi_heads.parameters(), #type: ignore\n",
    "                lr = 1e-4\n",
    "            )\n",
    "        raise AssertionError(\"Invalid training phase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune on new data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=.\n",
    "# Reload manually to update results (at first hit reload after 1st epoch is done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from copy import deepcopy\n",
    "\n",
    "# Initialize a Faster RCNN model\n",
    "model = FasterRCNNModel(num_classes=6)\n",
    "\n",
    "# Train in 4 phases as described in the original paper\n",
    "\n",
    "# Phase 0\n",
    "model.training_phase = 0\n",
    "\n",
    "# Save initial weights of the backbone CNN\n",
    "p0_backbone_state_dict = deepcopy(model.model.backbone.state_dict())\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\", \n",
    "    devices=1,\n",
    "    max_epochs=10,\n",
    "    default_root_dir=\"frcnn_p0\"\n",
    ")\n",
    "trainer.fit(model, train_dataloaders=train_dloader)\n",
    "\n",
    "\n",
    "# Phase 1\n",
    "\n",
    "# Restore initial weights of the backbone CNN\n",
    "model.model.backbone.load_state_dict(p0_backbone_state_dict)\n",
    "\n",
    "model.training_phase = 1\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\", \n",
    "    devices=1,\n",
    "    max_epochs=10,\n",
    "    default_root_dir=\"frcnn_p1\",\n",
    "    check_val_every_n_epoch=5\n",
    ")\n",
    "trainer.fit(model, train_dataloaders=train_dloader, val_dataloaders=val_dloader)\n",
    "\n",
    "# Phase 2\n",
    "model.training_phase = 2\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\", \n",
    "    devices=1,\n",
    "    max_epochs=10,\n",
    "    default_root_dir=\"frcnn_p2\"\n",
    ")\n",
    "trainer.fit(model, train_dataloaders=train_dloader)\n",
    "\n",
    "# Phase 3\n",
    "\n",
    "model.training_phase = 3\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\", \n",
    "    devices=1,\n",
    "    min_epochs=10,\n",
    "    max_epochs=100,\n",
    "    default_root_dir=\"frcnn_p3\",\n",
    "    check_val_every_n_epoch=2,\n",
    ")\n",
    "trainer.fit(model, train_dataloaders=train_dloader, val_dataloaders=val_dloader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
