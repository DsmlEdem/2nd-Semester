{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rslab-ntua/MSc_GBDA/2022/Lab2_simple_cnn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://madm.dfki.de/files/sentinel/EuroSATallBands.zip\n",
    "!unzip EuroSATallBands.zip\n",
    "!rm EuroSATallBands.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rasterio\n",
    "!pip install pytorch-lightning\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from glob import glob\n",
    "import os\n",
    "import rasterio\n",
    "from typing import Callable, List\n",
    "import numpy as np\n",
    "\n",
    "DATA_ROOT = \"ds/images/remote_sensing/otherDatasets/sentinel_2/tif/\"\n",
    "\n",
    "TransformFun = Callable[[dict], dict]\n",
    "\n",
    "class EuroSAT(Dataset):\n",
    "    def __init__(self, data_root, transforms: List[TransformFun] = []):\n",
    "        super().__init__()\n",
    "        self._build_db(data_root)\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def _build_db(self, data_root) -> None:\n",
    "        sample_urls = sorted(glob(os.path.join(data_root, \"**/*.tif\"), recursive=True))\n",
    "        \n",
    "        def parse_category(url):\n",
    "            return os.path.basename(os.path.dirname(url))\n",
    "        \n",
    "        # Get unique category names in alphabetical order\n",
    "        categories = sorted(list(set([parse_category(url) for url in sample_urls])))\n",
    "        self.categories = {c_name: idx for idx, c_name in enumerate(categories)}\n",
    "        \n",
    "        self.db = []\n",
    "        for s_url in sample_urls:\n",
    "            self.db.append({\n",
    "                \"url\": s_url,\n",
    "                \"category_name\": parse_category(s_url),\n",
    "                \"category_id\": self.categories[parse_category(s_url)]\n",
    "            })\n",
    "    \n",
    "    @property\n",
    "    def num_categories(self):\n",
    "        return(len(self.categories))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample =  self.db[index]\n",
    "        \n",
    "        for T in self.transforms:\n",
    "            sample = T(sample)\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "    \n",
    "        \n",
    "\n",
    "def load_data():\n",
    "    def apply(x:dict) -> dict:\n",
    "        assert \"url\" in x\n",
    "        with rasterio.open(x[\"url\"]) as dataset:\n",
    "            x.update({\"data\": dataset.read()})\n",
    "        return x\n",
    "    return apply\n",
    "\n",
    "def normalize(factor=10000):\n",
    "    def apply(x:dict) -> dict:\n",
    "        assert \"data\" in x\n",
    "        x[\"data\"] = x[\"data\"].astype(np.float32) / factor\n",
    "        return x\n",
    "    return apply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/val/test set\n",
    "dset = EuroSAT(data_root=DATA_ROOT, transforms=[load_data(), normalize()])\n",
    "\n",
    "train_dset, val_dset, test_dset = random_split(dset, \n",
    "        lengths=[\n",
    "            int(0.7*len(dset)),\n",
    "            int(0.2*len(dset)),\n",
    "            len(dset) - int(0.7*len(dset)) - int(0.2*len(dset))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "train_dloader =DataLoader(train_dset, batch_size=128, shuffle=True, num_workers=2)\n",
    "val_dloader =DataLoader(val_dset, batch_size=128, shuffle=False, num_workers=2)\n",
    "test_dloader =DataLoader(test_dset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple CNN\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import Accuracy, ConfusionMatrix\n",
    "from torchsummary import summary\n",
    "\n",
    "class CNN(pl.LightningModule):\n",
    "    def __init__(self, channels_in, num_classes, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(channels_in, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(output_size=(4, 4))\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4*4*128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        self.lr = lr\n",
    "\n",
    "        self.train_accuracy = Accuracy()\n",
    "\n",
    "        self.val_accuracy = Accuracy()\n",
    "        self.val_confusion_matrix = ConfusionMatrix(num_classes)\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X = batch[\"data\"]\n",
    "        y = batch[\"category_id\"]\n",
    "\n",
    "        logits = self(X)\n",
    "\n",
    "        loss = F.nll_loss(torch.log_softmax(logits, dim=-1), y)\n",
    "        self.log(\"loss/train\", loss, on_epoch=True, on_step=False)\n",
    "\n",
    "        self.train_accuracy(logits, y)\n",
    "        self.log(\"accuracy/train\", self.train_accuracy, on_epoch=True, on_step=False)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X = batch[\"data\"]\n",
    "        y = batch[\"category_id\"]\n",
    "\n",
    "        logits = self(X)\n",
    "\n",
    "        loss = F.nll_loss(torch.log_softmax(logits, dim=-1), y)\n",
    "        self.log(\"loss/val\", loss, on_epoch=True, on_step=False)\n",
    "\n",
    "        self.val_accuracy(logits, y)\n",
    "        self.log(\"accuracy/val\", self.val_accuracy, on_epoch=True, on_step=False)\n",
    "\n",
    "        self.val_confusion_matrix(logits, y)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "    \n",
    "print(summary(CNN(13, dset.num_categories), input_size=(13, 64, 64), device=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"accuracy/val\", mode=\"max\", patience=3),\n",
    "    ModelCheckpoint(monitor=\"accuracy/val\", mode=\"max\", save_last=True)\n",
    "]\n",
    "\n",
    "model = CNN(13, dset.num_categories)\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\", \n",
    "    devices=1,\n",
    "    max_epochs=20,\n",
    "    callbacks=callbacks,\n",
    "    default_root_dir=\"simple_cnn\"\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_dloader, val_dataloaders=val_dloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir simple_cnn/lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "best_model = CNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "trainer.validate(model, dataloaders=test_dloader)\n",
    "\n",
    "cm = model.val_confusion_matrix.compute().cpu().numpy()\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                               display_labels=dset.categories.keys())\n",
    "plt.figure(figsize=(20,20), dpi=100)\n",
    "ax = plt.axes()\n",
    "\n",
    "disp.plot(ax=ax)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
