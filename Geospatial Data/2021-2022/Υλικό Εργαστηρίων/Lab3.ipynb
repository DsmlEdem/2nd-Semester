{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rslab-ntua/MSc_GBDA/2022/Lab3.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!wget -P data/oxford-iiit-pet \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\"\n",
    "!wget -P data/oxford-iiit-pet \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\"\n",
    "\n",
    "!tar -xf data/oxford-iiit-pet/annotations.tar.gz --directory data/oxford-iiit-pet\n",
    "!tar -xf data/oxford-iiit-pet/images.tar.gz --directory data/oxford-iiit-pet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install albumentations\n",
    "!pip install pytorch-lightning\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Utility function for image grid plots\n",
    "def display_image_grid(images: List[torch.Tensor], masks: List[torch.Tensor], predicted_masks: Optional[List[torch.Tensor]] = None):\n",
    "    cols = 3 if predicted_masks is not None else 2\n",
    "    rows = len(images)\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(10, 24))\n",
    "    for i in range(rows):\n",
    "        ax[i, 0].imshow(images[i].cpu().numpy().transpose(1,2,0))\n",
    "        ax[i, 1].imshow(masks[i].cpu().numpy(), interpolation=\"nearest\")\n",
    "\n",
    "        ax[i, 0].set_title(\"Image\")\n",
    "        ax[i, 1].set_title(\"Ground truth mask\")\n",
    "\n",
    "        ax[i, 0].set_axis_off()\n",
    "        ax[i, 1].set_axis_off()\n",
    "\n",
    "        if predicted_masks is not None:\n",
    "            predicted_mask = predicted_masks[i]\n",
    "            ax[i, 2].imshow(predicted_mask.cpu().numpy(), interpolation=\"nearest\")\n",
    "            ax[i, 2].set_title(\"Predicted mask\")\n",
    "            ax[i, 2].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.io import read_image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import glob\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "DATA_ROOT = \"data/oxford-iiit-pet\"\n",
    "\n",
    "class OxfordPets(Dataset):\n",
    "    def __init__(self, data_root, transform=None, indices=None):\n",
    "        super().__init__()\n",
    "        self.indices = indices\n",
    "        self._build_db(data_root)\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "    \n",
    "    def _build_db(self, data_root):\n",
    "        self.db = []\n",
    "        \n",
    "        im_files = sorted(glob.glob(os.path.join(data_root, \"images\", \"*.jpg\")))\n",
    "        \n",
    "        def append_file(im_file):\n",
    "            \n",
    "            if cv2.imread(im_file) is None:\n",
    "                return\n",
    "            \n",
    "            im_name = os.path.splitext(os.path.basename(im_file))[0]\n",
    "            mask_file = os.path.join(data_root, \"annotations\", \"trimaps\", im_name + \".png\")\n",
    "            \n",
    "            sample = {\n",
    "                \"im_file\": im_file,\n",
    "                \"mask_file\": mask_file\n",
    "            }\n",
    "            self.db.append(sample)\n",
    "        \n",
    "        if self.indices is not None:\n",
    "            for idx in self.indices:\n",
    "                append_file(im_files[idx])\n",
    "        else:\n",
    "            for im_file in im_files:\n",
    "                append_file(im_file)\n",
    "            \n",
    "    \n",
    "    def preprocess_mask(self, mask):\n",
    "        mask = mask.astype(np.float32)\n",
    "        mask[mask == 2.0] = 0.0\n",
    "        mask[(mask == 1.0) | (mask == 3.0)] = 1.0\n",
    "        return mask\n",
    "\n",
    "    def _load_data(self, sample):\n",
    "        s = copy.copy(sample)\n",
    "        s.update({\n",
    "            \"im\": cv2.cvtColor(cv2.imread(s[\"im_file\"]), cv2.COLOR_BGR2RGB),\n",
    "            \"mask\": self.preprocess_mask(cv2.imread(s[\"mask_file\"], cv2.IMREAD_UNCHANGED))\n",
    "        })\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self._load_data(self.db[index])\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=sample[\"im\"], mask=sample[\"mask\"])\n",
    "            sample[\"im\"] = transformed[\"image\"]\n",
    "            sample[\"mask\"] = transformed[\"mask\"]\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        # A.Resize(256, 256),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dset = OxfordPets(DATA_ROOT, transform=transform)\n",
    "\n",
    "ims, masks = [], []\n",
    "for i in range(3):\n",
    "    ims.append(dset[i][\"im\"])\n",
    "    masks.append(dset[i][\"mask\"])\n",
    "\n",
    "display_image_grid(ims, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dset = OxfordPets(DATA_ROOT, transform=transform)\n",
    "all_indices = np.arange(len(dset))\n",
    "\n",
    "train_indices, val_indices = train_test_split(all_indices, test_size=0.3)\n",
    "\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(256, 256),\n",
    "        A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5),\n",
    "        A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "train_dset = OxfordPets(DATA_ROOT, transform=train_transform, indices=train_indices)\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [A.Resize(256, 256), \n",
    "     A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                 std=(0.229, 0.224, 0.225)),\n",
    "     ToTensorV2()]\n",
    ")\n",
    "val_dset = OxfordPets(DATA_ROOT, transform=val_transform, indices=val_indices)\n",
    "\n",
    "\n",
    "ims, masks = [], []\n",
    "for i in range(2):\n",
    "    ims.append(val_dset[i][\"im\"])\n",
    "    masks.append(val_dset[i][\"mask\"])\n",
    "\n",
    "print(len(train_dset), len(val_dset))\n",
    "display_image_grid(ims, masks)\n",
    "\n",
    "# Configure DataLoaders\n",
    "train_dloader = DataLoader(train_dset, batch_size=4, shuffle=True, num_workers=2)\n",
    "val_dloader = DataLoader(val_dset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.vgg import vgg11\n",
    "\n",
    "\n",
    "vgg = vgg11(pretrained=True, progress=False)\n",
    "print(vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torchsummary import summary\n",
    "from torch import nn\n",
    "from torchvision.models.vgg import vgg11\n",
    "from torch.nn import functional as F\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "\n",
    "class FCN(pl.LightningModule):\n",
    "    def __init__(self, lr=1e-3):\n",
    "        super().__init__()\n",
    "        \n",
    "        vgg = vgg11(pretrained=True)\n",
    "        self.encoder = nn.Sequential(*list(vgg.features.children())[:15])\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "            nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Flatten(0, 1)\n",
    "        )\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.pixel_accuracy = Accuracy()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        im = batch[\"im\"]\n",
    "        mask = batch[\"mask\"]\n",
    "        \n",
    "        preds = self(im)\n",
    "        \n",
    "        loss = F.binary_cross_entropy_with_logits(input=preds, target=mask)\n",
    "        \n",
    "        self.log(\"loss/train\", loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        im = batch[\"im\"]\n",
    "        mask = batch[\"mask\"]\n",
    "        \n",
    "        preds = self(im)\n",
    "        \n",
    "        loss = F.binary_cross_entropy_with_logits(input=preds, target=mask)\n",
    "        \n",
    "        self.log(\"loss/val\", loss, on_step=False, on_epoch=True)\n",
    "\n",
    "        self.pixel_accuracy(preds, mask.type(torch.long))\n",
    "        self.log(\"px_acc/val\", self.pixel_accuracy, on_step=False, on_epoch=True)\n",
    "        \n",
    "            \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "    \n",
    "print(summary(FCN(), input_size=(3,256,256), device=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"px_acc/val\", mode=\"max\", patience=3),\n",
    "    ModelCheckpoint(monitor=\"px_acc/val\", mode=\"max\", save_last=True)\n",
    "]\n",
    "\n",
    "model = FCN()\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\", \n",
    "    devices=1,\n",
    "    max_epochs=20,\n",
    "    callbacks=callbacks,\n",
    "    default_root_dir=\"fcn\"\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_dloader, val_dataloaders=val_dloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir fcn/lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with \"best\" model\n",
    "\n",
    "best_model = FCN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "best_model.eval()\n",
    "\n",
    "s = next(iter(val_dloader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    display_image_grid(s[\"im\"], s[\"mask\"], best_model(s[\"im\"]) > 0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
